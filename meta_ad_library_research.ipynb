{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducible setup\n",
    "\n",
    "This notebook was adapted for GitHub so it runs with **relative paths** and includes a small helper to keep data and outputs organized.\n",
    "\n",
    "**Structure**\n",
    "```\n",
    "thesis-graphs/\n",
    "├─ notebook/                # this notebook lives here\n",
    "├─ data/                    # CSV inputs (committed)\n",
    "├─ outputs/                 # CSV exports (gitignored)\n",
    "├─ figures/                 # saved figures (gitignored)\n",
    "├─ requirements.txt\n",
    "└─ README.md\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Project root = folder containing this notebook (two levels up if executed from repo root)\n",
    "ROOT = Path().resolve()\n",
    "if (ROOT / 'notebook').exists():\n",
    "    # executed from repo root\n",
    "    pass\n",
    "else:\n",
    "    # executed from inside notebook/ already; move up\n",
    "    while not (ROOT / 'data').exists() and ROOT.parent != ROOT:\n",
    "        ROOT = ROOT.parent\n",
    "\n",
    "DATA_DIR = ROOT / 'data'\n",
    "OUT_DIR  = ROOT / 'outputs'\n",
    "FIG_DIR  = ROOT / 'figures'\n",
    "for p in (DATA_DIR, OUT_DIR, FIG_DIR):\n",
    "    p.mkdir(exist_ok=True)\n",
    "\n",
    "# Input filenames (shipped in data/)\n",
    "META_ADS_CSV = DATA_DIR / 'meta-ad-library.csv'\n",
    "ELECTIONS_CSV = DATA_DIR / 'election-results.csv'\n",
    "\n",
    "print('Using data files:', META_ADS_CSV.name, 'and', ELECTIONS_CSV.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e35a6e-80d4-440a-ab7a-838d6ac4c868",
   "metadata": {
    "id": "f6e35a6e-80d4-440a-ab7a-838d6ac4c868"
   },
   "source": [
    "# Meta Ad Library Analysis #\n",
    "\n",
    "This notebook shows code used by Bellingcat to clean and analyse data from the Meta Ad Library from pages running ads sponsored by Die Linke and other affiliated pages seen by Facebook and Instagram users between January 1 and Feb. 23, 2025 in the lead up to the federal election 2025.\n",
    "\n",
    "This code uses the following CSV files:\n",
    "\n",
    "meta-ad-library.csv contains combined data from these advertisers/pages that ran ads sponsored by Die Linke' campaign.\n",
    "\n",
    "results_2025_German_election.csv containing the overall results from all areas (federal territory, federal states, and constituencies) in tabular form published by the Federal Returning Officer.\n",
    "\n",
    "It can also be repurposed to analyse other CSV files downloaded directly from the Meta Ad Library, as long as references to these files are added to data_files.\n",
    "\n",
    "This code was originally put together by Pooja Chaudhuri and Melissa Zhu, but is adjusted by Karla Hootz for the purpose of analysing the rise of Die Linke in the federal election 2025 in Germany."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d33f3a-4e9f-47bb-9fed-666322e1b7f8",
   "metadata": {
    "id": "03d33f3a-4e9f-47bb-9fed-666322e1b7f8"
   },
   "source": [
    "## Imports and basic functions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f040ec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25d2b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2191553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650b76c8-58fe-4ad2-a2f6-58e56b84b42b",
   "metadata": {
    "id": "650b76c8-58fe-4ad2-a2f6-58e56b84b42b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import unicodedata\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d69cb2-3a89-44a0-9c3f-bf7e870bb29e",
   "metadata": {
    "id": "a4d69cb2-3a89-44a0-9c3f-bf7e870bb29e"
   },
   "outputs": [],
   "source": [
    "# Define variables for each candidate's data; replace with appropriate filenames if replicating with other data\n",
    "\n",
    "data_files = {\n",
    "    str(META_ADS_CSV),\n",
    "    # add or replace as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1414c8-1dda-40e4-bcf4-17443a102ed0",
   "metadata": {
    "id": "2f1414c8-1dda-40e4-bcf4-17443a102ed0"
   },
   "outputs": [],
   "source": [
    "def add_average_spending(df):\n",
    "    '''Splits 'spend' column into its upper and lower limits, and calculates the average, adding a new column with this value'''\n",
    "\n",
    "    df[['lower_spend', 'upper_spend']] = df['spend'].str.extract(r'lower_bound:\\s*(\\d+),\\s*upper_bound:\\s*(\\d+)')\n",
    "\n",
    "    df['lower_spend'] = pd.to_numeric(df['lower_spend'])\n",
    "    df['upper_spend'] = pd.to_numeric(df['upper_spend'])\n",
    "\n",
    "    df['spend_average'] = (df['lower_spend'] + df['upper_spend']) / 2\n",
    "\n",
    "    print(f\"The total amount spent on all Meta ads in the data is about ${df['spend_average'].sum()} based on the average of the upper and lower limits for spending on each ad.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951bea54-0f1e-482c-99fd-c0904a9f9ac7",
   "metadata": {
    "id": "951bea54-0f1e-482c-99fd-c0904a9f9ac7"
   },
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "def process_delivery_by_region(df):\n",
    "    '''Takes the data in 'delivery_by_region', which is a dictionary showing the percentage of impressions from each region, and creates\n",
    "    a new column for each region, with the percentage of impressions for that region as the value.'''\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # Parse delivery_by_region and construct a list of rows with region and percentage data\n",
    "    for index, cell in df[['ad_archive_id', 'delivery_by_region']].dropna().iterrows():\n",
    "        try:\n",
    "            # Parse JSON-like data from the cell\n",
    "            parsed_data = json.loads(f'[{cell[\"delivery_by_region\"]}]')\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing cell: {cell['delivery_by_region']}\\n{e}\")\n",
    "            parsed_data = []\n",
    "\n",
    "        # Add ad_archive_id to each parsed entry\n",
    "        for entry in parsed_data:\n",
    "            entry['ad_archive_id'] = cell['ad_archive_id']\n",
    "            rows.append(entry)\n",
    "\n",
    "    # Create a DataFrame from the rows\n",
    "    regions_df = pd.DataFrame(rows)\n",
    "\n",
    "    # Pivot the regions DataFrame to have regions as columns\n",
    "    regions_pivot = regions_df.pivot_table(\n",
    "        index='ad_archive_id',\n",
    "        columns='region',\n",
    "        values='percentage',\n",
    "        aggfunc='first'\n",
    "    ).fillna(0) * 100  # Convert to percentage\n",
    "\n",
    "    # Clean up column names and reset index\n",
    "    regions_pivot.columns.name = None\n",
    "    regions_pivot = regions_pivot.reset_index()\n",
    "\n",
    "    # Merge the pivoted regions data back into the original DataFrame\n",
    "    df_merged = df.merge(regions_pivot, on='ad_archive_id', how='left')\n",
    "\n",
    "    # Remove rows with null delivery_by_region values\n",
    "    df_merged = df_merged.dropna(subset=['delivery_by_region'])\n",
    "\n",
    "    state_columns = ['Baden-W\\u00fcrttemberg', 'Bayern', 'Berlin', 'Brandenburg', 'Bremen',\n",
    "       'Hamburg', 'Hessen', 'Mecklenburg-Vorpommern', 'Niedersachsen', 'Nordrhein-Westfalen', 'Rheinland-Pfalz',\n",
    "       'Sachsen', 'Saxony-Anhalt', 'Schleswig-Holstein', 'Th\\u00fcringen', 'Saarland', 'Berlin (city)', 'Bremen (city)', 'Alsace']\n",
    "\n",
    "    # Print region names\n",
    "    region_names = [col for col in regions_pivot.columns if col != 'ad_archive_id']\n",
    "    non_de_regions = [col for col in region_names if col not in state_columns]\n",
    "    print(\"Region columns added:\")\n",
    "    print(region_names)\n",
    "    print()\n",
    "    print(\"This includes the following non-US regions:\")\n",
    "    print(non_de_regions)\n",
    "    print()\n",
    "\n",
    "    # Calculate the total non-US delivery percentage out of the overall total\n",
    "    non_de_total_percentage = df_merged[non_de_regions].sum().sum() / df_merged[region_names].sum().sum() * 100\n",
    "    print(f\"Total percentage of ad delivery in non-US regions across all ads: {non_de_total_percentage:.3f}%\")\n",
    "\n",
    "    return region_names, non_de_regions, df_merged\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(str(META_ADS_CSV))\n",
    "# Optional: call and visualization\n",
    "region_names, non_de_regions, df_with_regions = process_delivery_by_region(df)\n",
    "\n",
    "# Beispiel-Plot: Durchschnittliche Anteile der Top 10 Regionen\n",
    "region_means = df_with_regions[region_names].mean().sort_values(ascending=False).head(10)\n",
    "region_means.plot(kind=\"barh\")\n",
    "plt.xlabel(\"percentage\")\n",
    "plt.ylabel(\"region\")\n",
    "plt.title(\"Top 10 regional distribution by ad\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48526e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# --- Mapping (from our earlier step) ---\n",
    "city_to_state = {\n",
    "    \"Berlin\": \"Berlin\",\n",
    "    \"Hamburg\": \"Hamburg\",\n",
    "    \"Bielefeld\": \"Nordrhein-Westfalen\",\n",
    "    \"Dresden\": \"Sachsen\",\n",
    "    \"Hildesheim\": \"Niedersachsen\",\n",
    "    \"Erfurt\": \"Thüringen\",\n",
    "    \"Offenbach\": \"Hessen\",\n",
    "    \"Leipzig\": \"Sachsen\",\n",
    "}\n",
    "\n",
    "# Regex patterns to detect city names in text\n",
    "city_patterns = {city: re.compile(rf\"\\b{city}\\b\", re.IGNORECASE) for city in city_to_state.keys()}\n",
    "\n",
    "# --- Add boolean flags for city mentions in ad_creative_bodies ---\n",
    "for city, pattern in city_patterns.items():\n",
    "    df_with_regions[f\"mention_{city}\"] = df_with_regions[\"ad_creative_bodies\"].astype(str).str.contains(pattern)\n",
    "\n",
    "# --- Analysis: compare delivery in matching Bundesland ---\n",
    "results = []\n",
    "\n",
    "for city, state in city_to_state.items():\n",
    "    mention_mask = df_with_regions[f\"mention_{city}\"]\n",
    "\n",
    "    if state in df_with_regions.columns:\n",
    "        avg_with = df_with_regions.loc[mention_mask, state].mean()\n",
    "        avg_without = df_with_regions.loc[~mention_mask, state].mean()\n",
    "        results.append({\n",
    "            \"City\": city,\n",
    "            \"State\": state,\n",
    "            \"Mentions\": mention_mask.sum(),\n",
    "            \"AvgDelivery_withMention\": avg_with,\n",
    "            \"AvgDelivery_withoutMention\": avg_without,\n",
    "            \"Difference\": avg_with - avg_without\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"Difference\", ascending=False)\n",
    "\n",
    "print(\"Correlation between city mentions in text and delivery by region:\")\n",
    "print(results_df)\n",
    "\n",
    "# --- Optional Plot ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(results_df[\"City\"], results_df[\"Difference\"], color=\"steelblue\")\n",
    "plt.axvline(0, color=\"black\", linestyle=\"--\")\n",
    "plt.xlabel(\"Difference in average share (%)\")\n",
    "plt.title(\"Greater reach in the region when the city is mentioned in the ad text\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f2d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Define pattern for Berlin (include district mentions if needed)\n",
    "berlin_pattern = re.compile(r\"\\bberlin\\b|\\bneukölln\\b\", flags=re.IGNORECASE)\n",
    "\n",
    "# Create binary indicator: does the ad mention Berlin?\n",
    "df_with_regions[\"mention_berlin\"] = df_with_regions[\"ad_creative_bodies\"].astype(str).str.contains(berlin_pattern)\n",
    "\n",
    "# Build DataFrame for plotting\n",
    "df_plot = pd.DataFrame({\n",
    "    \"Delivery %\": pd.to_numeric(df_with_regions[\"Berlin\"], errors=\"coerce\"),\n",
    "    \"Mention Berlin\": df_with_regions[\"mention_berlin\"].map({True: \"Yes\", False: \"No\"})\n",
    "}).dropna()\n",
    "\n",
    "# Plot boxplot\n",
    "plt.figure(figsize=(6,4))\n",
    "df_plot.boxplot(column=\"Delivery %\", by=\"Mention Berlin\", grid=False)\n",
    "plt.suptitle(\"\")\n",
    "plt.title(\"Berlin delivery share by text mention\")\n",
    "plt.ylabel(\"Delivery (%)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5bf283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build df_demographics from 'demographic_distribution' ---\n",
    "def safe_json_loads(s):\n",
    "    if not isinstance(s, str):\n",
    "        return []\n",
    "    s = s.replace(\"'\", '\"')\n",
    "    if not s.strip().startswith(\"[\"):\n",
    "        s = \"[\" + s + \"]\"\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "df_with_regions[\"demographic_distribution\"] = df_with_regions[\"demographic_distribution\"].apply(safe_json_loads)\n",
    "\n",
    "df_exploded = df_with_regions.explode(\"demographic_distribution\")\n",
    "\n",
    "df_demographics = pd.concat(\n",
    "    [\n",
    "        df_exploded[[\"ad_archive_id\"]].reset_index(drop=True),\n",
    "        pd.json_normalize(df_exploded[\"demographic_distribution\"]).reset_index(drop=True)\n",
    "    ],\n",
    "    axis=1\n",
    ").dropna(subset=[\"ad_archive_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8e8a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= Microtargeting Index (uses SAME notebook definitions) =================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Utilities\n",
    "def _safe_minmax(x: pd.Series) -> pd.Series:\n",
    "    x = x.astype(float)\n",
    "    if x.max() == x.min():\n",
    "        return pd.Series(0.0, index=x.index)\n",
    "    return (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "def _hhi(shares) -> float:\n",
    "    \"\"\"Herfindahl over shares (expects non-negative; normalizes to sum=1).\"\"\"\n",
    "    arr = np.asarray(shares, dtype=float)\n",
    "    arr = arr[np.isfinite(arr)]\n",
    "    arr = arr[arr >= 0]\n",
    "    s = arr.sum()\n",
    "    if s <= 0:\n",
    "        return 0.0\n",
    "    p = arr / s\n",
    "    return float(np.sum(p * p))\n",
    "\n",
    "# Local terms kept consistent with earlier text analysis\n",
    "GERMAN_STATES = [\n",
    "    \"Baden-Wurttemberg\", \"Bavaria\", \"Berlin\", \"Brandenburg\", \"Bremen\", \"Hamburg\", \"Hesse\",\n",
    "    \"Mecklenburg-Western Pomerania\", \"Lower Saxony\", \"North Rhine-Westphalia\", \"Rhineland-Palatinate\",\n",
    "    \"Saarland\", \"Saxony\", \"Saxony-Anhalt\", \"Schleswig-Holstein\", \"Thuringia\"\n",
    "]\n",
    "CITIES_CORE = [\"Berlin\", \"Hamburg\", \"Bielefeld\", \"Dresden\", \"Hildesheim\", \"Erfurt\", \"Offenbach\", \"Leipzig\"]\n",
    "DISTRICT_TO_CITY = {\"Neukoelln\": \"Berlin\"}  # extend as needed\n",
    "\n",
    "LOCAL_TERMS = set(GERMAN_STATES) | set(CITIES_CORE) | set(DISTRICT_TO_CITY.keys())\n",
    "TERM_REGEX = {t: re.compile(r\"(?<![\\wÄÖÜäöüß])\" + re.escape(t) + r\"(?![\\wÄÖÜäöüß])\", re.IGNORECASE)\n",
    "              for t in LOCAL_TERMS}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ac5c5-75b7-4f0a-8ca5-f548dba09321",
   "metadata": {
    "id": "e95ac5c5-75b7-4f0a-8ca5-f548dba09321"
   },
   "source": [
    "## Describing the data and processing it for further analysis ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca255d4-fdfb-46ec-8250-785c43e68f81",
   "metadata": {
    "id": "4ca255d4-fdfb-46ec-8250-785c43e68f81"
   },
   "outputs": [],
   "source": [
    "processed_data = {} # initialising empty dictionary to save processed DataFrames\n",
    "region_names_dict = {} # to save region names in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3480ae-e3d3-4f7f-9a49-40f99ab486f9",
   "metadata": {
    "id": "5c3480ae-e3d3-4f7f-9a49-40f99ab486f9",
    "outputId": "0dbfbc1a-00a2-4526-b4bd-61530bce572b"
   },
   "outputs": [],
   "source": [
    "# Run the same functions on all files in the data_files\n",
    "\n",
    "for name, file in data_files.items():\n",
    "\n",
    "    print(f\"### PROCESSING META AD DATA FOR {name.upper()} ###\")\n",
    "    print()\n",
    "\n",
    "    df = pd.read_csv(file) # read the data file as a DataFrame\n",
    "\n",
    "    print(f\"This dataset contains {len(df)} ads from {len(df['page_name'].unique())} pages.\")\n",
    "    print()\n",
    "    print(\"The pages included are:\")\n",
    "    print()\n",
    "    print(df['page_name'].unique())\n",
    "    print()\n",
    "\n",
    "    df = add_average_spending(df)\n",
    "    print()\n",
    "\n",
    "    region_names, df_merged = process_delivery_by_region(df)\n",
    "    print()\n",
    "\n",
    "    region_names_dict[name] = region_names\n",
    "\n",
    "    processed_data[name] = df_merged\n",
    "\n",
    "    print(f\"DataFrame saved as processed_data['{name}'].\")\n",
    "    print(f\"Names of region columns added saved as region_names_dict['{name}'].\")\n",
    "    print()\n",
    "\n",
    "    print(\"-------------------------------------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981ecf74-16e2-4640-a838-a82bda9c72c9",
   "metadata": {
    "id": "981ecf74-16e2-4640-a838-a82bda9c72c9"
   },
   "source": [
    "## Further analysis ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55a11ec-0b9d-4b0f-8d2f-af9c78e8192a",
   "metadata": {
    "id": "f55a11ec-0b9d-4b0f-8d2f-af9c78e8192a"
   },
   "outputs": [],
   "source": [
    "def get_spending_by_platform(name, df):\n",
    "    '''Calculate the total spend for ads running on each platform and export as a CSV file.'''\n",
    "    total_spend = df['spend_average'].sum()\n",
    "\n",
    "    # Filtering data based on publisher platforms\n",
    "    df_fb_insta = df[df['publisher_platforms'] == 'facebook,instagram']\n",
    "    df_fb = df[df['publisher_platforms'] == 'facebook']\n",
    "    df_insta = df[df['publisher_platforms'] == 'instagram']\n",
    "\n",
    "    print(f\"{round(len(df_fb_insta)/len(df) * 100, 2)}% of ads for {name.capitalize()}'s campaign ran on both Facebook and Instagram during this period.\")\n",
    "    print(f\"{round(len(df_fb)/len(df) * 100, 2)}% only ran on Facebook, and {round(len(df_insta)/len(df) * 100, 2)}% only ran on Instagram.\")\n",
    "    print()\n",
    "\n",
    "    # Calculating spending per platform\n",
    "    fb_insta_spend = df_fb_insta['spend_average'].sum()\n",
    "    fb_spend = df_fb['spend_average'].sum()\n",
    "    insta_spend = df_insta['spend_average'].sum()\n",
    "    total_spend = fb_insta_spend + fb_spend + insta_spend\n",
    "\n",
    "    # Print amounts\n",
    "    print(f\"{name.capitalize()}'s campaign spent an estimated ${fb_insta_spend} ({round(fb_insta_spend/total_spend * 100, 2)}%) on ads that ran on both Facebook and Instagram during this period.\")\n",
    "    print(f\"It also spent an estimated ${fb_spend} on ads that only ran on Facebook ({round(fb_spend/total_spend * 100, 2)}%), and ${insta_spend} ({round(insta_spend/total_spend * 100, 2)}%) on ads that only ran on Instagram.\")\n",
    "    print(f\"This added up to ${total_spend} in total.\")\n",
    "    print()\n",
    "\n",
    "    # Check if the sum of platform spending equals the total spend. If not, raise error\n",
    "    calculated_total = fb_insta_spend + fb_spend + insta_spend\n",
    "    assert abs(calculated_total - total_spend) < 1e-6, \\\n",
    "        f\"Calculated spending ({calculated_total}) does not match total spend ({total_spend})\"\n",
    "\n",
    "    # Creating a DataFrame with the spending data\n",
    "    spending_df = pd.DataFrame({\n",
    "        'Platform': ['Facebook & Instagram', 'Facebook', 'Instagram', 'Total'],\n",
    "        'Total_Spend': [fb_insta_spend, fb_spend, insta_spend, total_spend]\n",
    "    })\n",
    "\n",
    "    # Print the spending DataFrame\n",
    "    os.makedirs(\"data_for_vizzes/spending_by_platform\", exist_ok=True)\n",
    "    spending_df.to_csv(f\"data_for_vizzes/spending_by_platform/spending_by_platform_{name}.csv\", index=False)\n",
    "   \n",
    "    print(f\"Spending data has been exported to 'data_for_vizzes/spending_by_platform/spending_by_platform_{name}.csv'\")\n",
    "    # Exporting the spending data to a CSV file\n",
    "    spending_df.to_csv(f\"data_for_vizzes/spending_by_platform/spending_by_platform_{name}.csv\", index=False)\n",
    "    print(f\"Spending data has been exported to 'data_for_vizzes/spending_by_platform/_{name}.csv'\")\n",
    "\n",
    "    return spending_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d50aa64-2bc6-432c-9577-b7d501f69afa",
   "metadata": {
    "id": "9d50aa64-2bc6-432c-9577-b7d501f69afa"
   },
   "outputs": [],
   "source": [
    "#Estimate spending by state\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def _ensure_spend_average(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Ensure df has a numeric 'spend_average' column. If absent, derive midpoint from 'spend' ranges.\"\"\"\n",
    "    if \"spend_average\" in df.columns:\n",
    "        return df\n",
    "    if \"spend\" in df.columns:\n",
    "        def _mid(s):\n",
    "            if pd.isna(s):\n",
    "                return 0.0\n",
    "            nums = re.findall(r\"\\d+\", str(s))\n",
    "            if len(nums) >= 2:\n",
    "                lo, hi = map(int, nums[:2])\n",
    "                return (lo + hi) / 2.0\n",
    "            elif len(nums) == 1:\n",
    "                return float(nums[0])\n",
    "            return 0.0\n",
    "        df = df.copy()\n",
    "        df[\"spend_average\"] = df[\"spend\"].apply(_mid)\n",
    "        return df\n",
    "    raise KeyError(\"Neither 'spend_average' nor 'spend' found in the DataFrame.\")\n",
    "\n",
    "\n",
    "def estimate_spending_by_state(name: str, df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Estimate ad spending in each DE state by multiplying spend_average by delivery percentages for each state.\n",
    "    - name: label for file output (e.g., 'die_linke')\n",
    "    - df: DataFrame that includes state percentage columns (0..100) and spend_average (EUR)\n",
    "    \"\"\"\n",
    "    # candidates incl. variants we've seen in your data\n",
    "    state_candidates = [\n",
    "        'Baden-Württemberg','Bayern','Berlin','Brandenburg','Bremen','Hamburg','Hessen',\n",
    "        'Mecklenburg-Vorpommern','Niedersachsen','Nordrhein-Westfalen','Rheinland-Pfalz',\n",
    "        'Saarland','Sachsen','Sachsen-Anhalt','Schleswig-Holstein','Thüringen',\n",
    "        # extras seen in your dataset\n",
    "        'Saxony-Anhalt','Berlin (city)','Bremen (city)','Alsace'\n",
    "    ]\n",
    "\n",
    "    df = _ensure_spend_average(df)\n",
    "\n",
    "    # Keep only the state columns that actually exist\n",
    "    available_states = [s for s in state_candidates if s in df.columns]\n",
    "    if not available_states:\n",
    "        raise ValueError(\"No matching state columns were found in the DataFrame.\")\n",
    "\n",
    "    # Make sure state columns are numeric (coerce invalid to 0)\n",
    "    df_num = df.copy()\n",
    "    for s in available_states:\n",
    "        df_num[s] = pd.to_numeric(df_num[s], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # Compute estimated spend per state\n",
    "    state_spending = {}\n",
    "    for s in available_states:\n",
    "        state_spending[s] = (df_num[\"spend_average\"] * (df_num[s] / 100.0)).sum()\n",
    "\n",
    "    state_spending_df = (\n",
    "        pd.DataFrame({\"State\": list(state_spending.keys()), \"Estimated_Spend\": list(state_spending.values())})\n",
    "        .sort_values(\"Estimated_Spend\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Export\n",
    "    outdir = \"data_for_vizzes/spending_by_state\"\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    outpath = os.path.join(outdir, f\"estimated_spending_by_state_{name}.csv\")\n",
    "    state_spending_df.to_csv(outpath, index=False)\n",
    "    print(f\"Saved: {outpath}\")\n",
    "\n",
    "    # Top-3 message\n",
    "    top3 = state_spending_df.head(3)[\"State\"].tolist()\n",
    "    print(f\"{name.capitalize()} spent the most in: {', '.join(top3)}\")\n",
    "\n",
    "    # ---- Plot ----\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(state_spending_df[\"State\"], state_spending_df[\"Estimated_Spend\"])\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.ylabel(\"Estimated Spend (EUR)\")\n",
    "    plt.title(f\"Estimated Spending by State — {name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return state_spending_df\n",
    "\n",
    "result_df = estimate_spending_by_state(\"die_linke\", df_with_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a73574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Load your two prepared datasets ---\n",
    "spending_file = \"data_for_vizzes/spending_by_state/estimated_spending_by_state_die_linke.csv\"\n",
    "results_file = str(OUT_DIR / 'die_linke_regions_2025.csv')\n",
    "\n",
    "spending = pd.read_csv(spending_file)\n",
    "results = pd.read_csv(results_file)\n",
    "\n",
    "# Harmonize column names\n",
    "spending = spending.rename(columns={\"State\": \"Region\"})\n",
    "results = results.rename(columns={\"DieLinke_Zweitstimmen_Endgueltig\": \"Votes\"})\n",
    "\n",
    "# Merge datasets on Region\n",
    "merged = pd.merge(spending, results, on=\"Region\", how=\"inner\")\n",
    "\n",
    "# --- Scatter plot ---\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(\n",
    "    data=merged,\n",
    "    x=\"Estimated_Spend\", \n",
    "    y=\"Votes\", \n",
    "    hue=\"Region\", \n",
    "    s=100\n",
    ")\n",
    "\n",
    "# Fit regression line for overall trend\n",
    "sns.regplot(\n",
    "    data=merged,\n",
    "    x=\"Estimated_Spend\", \n",
    "    y=\"Votes\",\n",
    "    scatter=False,\n",
    "    color=\"black\",\n",
    "    line_kws={\"linestyle\": \"--\"}\n",
    ")\n",
    "\n",
    "plt.title(\"Ad Spending vs. Die Linke Votes (2025, by Bundesland)\")\n",
    "plt.xlabel(\"Estimated Spending (€)\")\n",
    "plt.ylabel(\"Die Linke Votes (Zweitstimme)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Correlation for context ---\n",
    "pearson_corr = merged[\"Estimated_Spend\"].corr(merged[\"Votes\"], method=\"pearson\")\n",
    "spearman_corr = merged[\"Estimated_Spend\"].corr(merged[\"Votes\"], method=\"spearman\")\n",
    "\n",
    "print(f\"Correlation between spending and votes:\")\n",
    "print(f\" Pearson r = {pearson_corr:.3f}\")\n",
    "print(f\" Spearman ρ = {spearman_corr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3e2eb3-dec4-40b3-9405-517dd1681536",
   "metadata": {
    "id": "ca3e2eb3-dec4-40b3-9405-517dd1681536"
   },
   "outputs": [],
   "source": [
    "def drop_empty_ad_text(df):\n",
    "    '''Removes all rows that do not have text in the 'ad_creative_bodies' column, for example video ads, for the purposes of text analysis.'''\n",
    "\n",
    "    df_clean = df.dropna(subset=['ad_creative_bodies'], how='all')\n",
    "    unique_ads_text = df_clean['ad_creative_bodies'].unique()\n",
    "\n",
    "    print(f\"Of the ads in the data, {len(df)-len(df_clean)} did not have any text accompanying the ads – for example video ads.\")\n",
    "    print(f\"For textual analysis, we will only look at the remaining {len(df_clean)} ads that have text, including {len(unique_ads_text)} unique ones.\")\n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22d3884-4e9c-4140-80f4-a2c552a4d906",
   "metadata": {
    "id": "f22d3884-4e9c-4140-80f4-a2c552a4d906"
   },
   "outputs": [],
   "source": [
    "# Initialise dictionary to store word combinations\n",
    "top_bigrams = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1177a95-1685-4621-9623-1f7b40d24b6e",
   "metadata": {
    "id": "f1177a95-1685-4621-9623-1f7b40d24b6e"
   },
   "outputs": [],
   "source": [
    "def normalise_unicode(text):\n",
    "    '''Normalise any stylised Unicode text to standard alphabetic characters'''\n",
    "\n",
    "    normalised_text = ''.join(\n",
    "        unicodedata.normalize('NFKD', c)[0] if 'MATHEMATICAL' in unicodedata.name(c, '') else c\n",
    "        for c in text\n",
    "    )\n",
    "    return normalised_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd15a4b-2ac1-4b2f-add2-fae1d3522f0a",
   "metadata": {
    "id": "ebd15a4b-2ac1-4b2f-add2-fae1d3522f0a"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    '''Remove times in formats like '4:00PM', '12:30AM', etc.'''\n",
    "\n",
    "    text = re.sub(r'\\b\\d{1,2}:\\d{2}\\s?(AM|PM|am|pm)?\\b', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6ec857-1952-4852-a5ae-b42b6cb70f67",
   "metadata": {
    "id": "2c6ec857-1952-4852-a5ae-b42b6cb70f67"
   },
   "outputs": [],
   "source": [
    "# Download stopwords, i.e. common words like 'the'\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973565ff-d7a5-4719-865a-2ae8844a8fdd",
   "metadata": {
    "id": "973565ff-d7a5-4719-865a-2ae8844a8fdd",
    "outputId": "690a834c-c37f-4c20-fde9-ac21f66b3300",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the same functions to analyse all of the data files\n",
    "\n",
    "for name, df in processed_data.items():\n",
    "\n",
    "    print(f\"### ANALYSING META AD DATA FOR {name.upper()} ###\")\n",
    "    print()\n",
    "\n",
    "    spending_df = get_spending_by_platform(name, df)\n",
    "    print()\n",
    "\n",
    "    state_spending_df = estimate_spending_by_state(name, df)\n",
    "    print()\n",
    "\n",
    "    df_clean = drop_empty_ad_text(df)\n",
    "    print()\n",
    "\n",
    "    label_ctas(name, df_clean)\n",
    "    print()\n",
    "\n",
    "    get_most_frequent_phrases(name, df_clean)\n",
    "    print()\n",
    "\n",
    "    print(\"-------------------------------------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946d3706-45b6-48c8-934c-a678b2d5bd9a",
   "metadata": {
    "id": "946d3706-45b6-48c8-934c-a678b2d5bd9a"
   },
   "outputs": [],
   "source": [
    "#demographic_distribution_analysis.py\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "csv_path = str(META_ADS_CSV)\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "def safe_json_loads(s):\n",
    "    if not isinstance(s, str):\n",
    "        return []\n",
    "    # Replace single quotes with double quotes\n",
    "    s = s.replace(\"'\", '\"')\n",
    "    # Add brackets if missing\n",
    "    if not s.strip().startswith(\"[\"):\n",
    "        s = \"[\" + s + \"]\"\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing: {s}\\n{e}\")\n",
    "        return []\n",
    "\n",
    "df[\"demographic_distribution\"] = df[\"demographic_distribution\"].apply(safe_json_loads)\n",
    "\n",
    "df_exploded = df.explode(\"demographic_distribution\")   \n",
    "\n",
    "df_demographics = pd.concat([df_exploded.drop(columns=[\"demographic_distribution\"]).reset_index(drop=True),pd.json_normalize(df_exploded[\"demographic_distribution\"]).reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(df_demographics)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Group by age group and gender and compute the mean\n",
    "pivot = df_demographics.groupby([\"age\", \"gender\"])['percentage'].mean().unstack()\n",
    "\n",
    "# Sort age groups logically (optional, if needed)\n",
    "# age_order = [\"18-24\", \"25-34\", \"35-44\", \"45-54\", \"55-64\", \"65+\"]\n",
    "# pivot = pivot.reindex(age_order)\n",
    "\n",
    "# Balkendiagramm zeichnen\n",
    "pivot.plot(kind=\"barh\")\n",
    "plt.xlabel(\"Average share\")\n",
    "plt.ylabel(\"age group\")\n",
    "plt.title(\"Average demographic distribution by age group and gender\")\n",
    "plt.legend(title=\"gender\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Save the processed DataFrame to a CSV file\n",
    "output_csv_path = \"data_for_vizzes/demographic_distribution.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff574d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship between spend and impressions\n",
    "def extract_midpoint(value):\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    try:\n",
    "        bounds = re.findall(r\"\\d+\", value)\n",
    "        if len(bounds) == 2:\n",
    "            lower = int(bounds[0])\n",
    "            upper = int(bounds[1])\n",
    "            return (lower + upper) / 2\n",
    "        elif len(bounds) == 1:\n",
    "            return int(bounds[0])\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Midpoints berechnen\n",
    "df[\"impressions_mid\"] = df[\"impressions\"].apply(extract_midpoint)\n",
    "df[\"spend_mid\"] = df[\"spend\"].apply(extract_midpoint)\n",
    "\n",
    "# Keep only valid rows\n",
    "valid_rows = df.dropna(subset=[\"impressions_mid\", \"spend_mid\"])\n",
    "\n",
    "# Korrelation berechnen\n",
    "correlation = valid_rows[\"impressions_mid\"].corr(valid_rows[\"spend_mid\"])\n",
    "print(f\"Korrelationskoeffizient zwischen Spend und Impressions: {correlation:.3f}\")\n",
    "\n",
    "# Scatter plot with log–log scale\n",
    "if (valid_rows[\"spend_mid\"] > 0).all() and (valid_rows[\"impressions_mid\"] > 0).all():\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(valid_rows[\"spend_mid\"], valid_rows[\"impressions_mid\"], alpha=0.5)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"Spend (Midpoint EUR, log scale)\")\n",
    "    plt.ylabel(\"Impressions (Midpoint, log scale)\")\n",
    "    plt.title(\"Correlation between spend and impressions\")\n",
    "    plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(valid_rows[\"spend_mid\"], valid_rows[\"impressions_mid\"], alpha=0.5)\n",
    "    plt.xlabel(\"Spend (Midpoint EUR)\")\n",
    "    plt.ylabel(\"Impressions (Midpoint)\")\n",
    "    plt.title(\"Correlation between spend and impressions\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d550b8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse voting data for the party \"Die Linke\" in the 2025 Bundestag elections\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the file (adjust if needed)\n",
    "vote_file = \"/Users/karlahootz/Desktop/Jura/LLM/Bocconi-Kings College/Material Kings/Dissertation/case study/btw25_rws_bst2.csv\"\n",
    "\n",
    "# Read CSV, skip metadata rows, use ';' as delimiter and latin-1 encoding\n",
    "votes_raw = pd.read_csv(vote_file, sep=\";\", skiprows=11, encoding=\"latin-1\")\n",
    "\n",
    "# Recode gender: w -> female, m|d|o -> male_other, Summe -> all\n",
    "votes_raw[\"gender\"] = votes_raw[\"Geschlecht\"].replace({\n",
    "    \"w\": \"female\",\n",
    "    \"m|d|o\": \"male_other\",\n",
    "    \"Summe\": \"all\"\n",
    "})\n",
    "\n",
    "# Keep only female and male_other\n",
    "votes = votes_raw[votes_raw[\"gender\"].isin([\"female\", \"male_other\"])].copy()\n",
    "\n",
    "# Select only relevant columns (age, gender, votes for Die Linke, total)\n",
    "votes = votes[[\"Geburtsjahresgruppe\", \"gender\", \"Die Linke\", \"Summe\"]].rename(\n",
    "    columns={\"Geburtsjahresgruppe\": \"age_group\", \"Die Linke\": \"DIE LINKE\"}\n",
    ")\n",
    "\n",
    "print(votes.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28203fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1) Load election CSV (header after metadata) ---\n",
    "vote_file = \"/Users/karlahootz/Desktop/Jura/LLM/Bocconi-Kings College/Material Kings/Dissertation/case study/btw25_rws_bst2.csv\"\n",
    "\n",
    "votes_raw = pd.read_csv(vote_file, sep=\";\", skiprows=11, encoding=\"latin-1\")\n",
    "\n",
    "# Only second vote (party preference)\n",
    "votes_raw = votes_raw[votes_raw[\"Erst-/Zweitstimme\"] == 2].copy()\n",
    "\n",
    "# Geschlecht harmonisieren\n",
    "votes_raw[\"gender\"] = votes_raw[\"Geschlecht\"].replace({\n",
    "    \"w\": \"female\",\n",
    "    \"m|d|o\": \"male_other\",\n",
    "    \"Summe\": \"all\"\n",
    "})\n",
    "\n",
    "# Age cohorts -> ad age buckets\n",
    "age_map = {\n",
    "    \"2001-2007\": \"18-24\",\n",
    "    \"1991-2000\": \"25-34\",\n",
    "    \"1981-1990\": \"35-44\",\n",
    "    \"1966-1980\": \"45-54\",\n",
    "    \"1956-1965\": \"55-64\",\n",
    "    \"<=1955\": \"65+\",\n",
    "    \"≤1955\": \"65+\"  # manche Dateien nutzen das ≤-Zeichen\n",
    "}\n",
    "votes_raw[\"age_group\"] = votes_raw[\"Geburtsjahresgruppe\"].map(age_map)\n",
    "\n",
    "# Nur weiblich & male_other behalten\n",
    "votes = votes_raw[votes_raw[\"gender\"].isin([\"female\",\"male_other\"])].copy()\n",
    "\n",
    "# Stimmenanteil DIE LINKE je Alters×Geschlecht\n",
    "# Note: column name must match your file exactly (\"Die Linke\")\n",
    "if \"Die Linke\" not in votes.columns:\n",
    "    raise KeyError(\"Spalte 'Die Linke' nicht gefunden. Prüfe die genaue Schreibweise der Parteispalte.\")\n",
    "\n",
    "votes_grp = (\n",
    "    votes.groupby([\"age_group\",\"gender\"], dropna=False)[[\"Die Linke\",\"Summe\"]]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "votes_grp = votes_grp.dropna(subset=[\"age_group\"])  # nur gemappte Altersgruppen\n",
    "votes_grp[\"vote_share\"] = votes_grp[\"Die Linke\"] / votes_grp[\"Summe\"]\n",
    "\n",
    "# --- 2) Map ad impression shares to the same groups ---\n",
    "# Expects df_demographics with columns: age (e.g., '18-24'), gender ('female','male','unknown'), percentage (0..1)\n",
    "ads_long = df_demographics.copy()\n",
    "\n",
    "# weiblich direkt\n",
    "ads_female = (\n",
    "    ads_long[ads_long[\"gender\"] == \"female\"]\n",
    "    .groupby(\"age\", as_index=False)[\"percentage\"]\n",
    "    .mean()\n",
    "    .rename(columns={\"age\":\"age_group\", \"percentage\":\"ad_share_female\"})\n",
    ")\n",
    "\n",
    "# male_other = male + unknown\n",
    "ads_male = (\n",
    "    ads_long[ads_long[\"gender\"] == \"male\"]\n",
    "    .groupby(\"age\", as_index=False)[\"percentage\"]\n",
    "    .mean()\n",
    "    .rename(columns={\"age\":\"age_group\", \"percentage\":\"ad_share_male\"})\n",
    ")\n",
    "ads_unknown = (\n",
    "    ads_long[ads_long[\"gender\"] == \"unknown\"]\n",
    "    .groupby(\"age\", as_index=False)[\"percentage\"]\n",
    "    .mean()\n",
    "    .rename(columns={\"age\":\"age_group\", \"percentage\":\"ad_share_unknown\"})\n",
    ")\n",
    "\n",
    "ads_comb = (\n",
    "    ads_female\n",
    "    .merge(ads_male, on=\"age_group\", how=\"outer\")\n",
    "    .merge(ads_unknown, on=\"age_group\", how=\"outer\")\n",
    "    .fillna(0.0)\n",
    ")\n",
    "ads_comb[\"ad_share_male_other\"] = ads_comb[\"ad_share_male\"] + ads_comb[\"ad_share_unknown\"]\n",
    "\n",
    "# in Long-Form bringen (age_group, gender, ad_share)\n",
    "ads_long2 = pd.concat([\n",
    "    ads_comb[[\"age_group\",\"ad_share_female\"]].assign(gender=\"female\").rename(columns={\"ad_share_female\":\"ad_share\"}),\n",
    "    ads_comb[[\"age_group\",\"ad_share_male_other\"]].assign(gender=\"male_other\").rename(columns={\"ad_share_male_other\":\"ad_share\"}),\n",
    "], ignore_index=True)\n",
    "\n",
    "# --- 3) Merge: Votes vs Ads ---\n",
    "merged = votes_grp.merge(ads_long2, on=[\"age_group\",\"gender\"], how=\"inner\")\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# Drop NaNs just in case\n",
    "df_corr = merged.dropna(subset=[\"vote_share\", \"ad_share\"])\n",
    "\n",
    "# Pearson correlation (linear)\n",
    "pearson_r, pearson_p = pearsonr(df_corr[\"vote_share\"], df_corr[\"ad_share\"])\n",
    "\n",
    "# Spearman correlation (rank-based, monotonic)\n",
    "spearman_r, spearman_p = spearmanr(df_corr[\"vote_share\"], df_corr[\"ad_share\"])\n",
    "\n",
    "print(\"Correlation between vote share (Die Linke) and ad impressions:\")\n",
    "print(f\" Pearson r = {pearson_r:.3f} (p={pearson_p:.4f})\")\n",
    "print(f\" Spearman ρ = {spearman_r:.3f} (p={spearman_p:.4f})\")\n",
    "\n",
    "\n",
    "# --- 4) Scatterplot ---\n",
    "plt.figure(figsize=(7,7))\n",
    "for g in merged[\"gender\"].unique():\n",
    "    sub = merged[merged[\"gender\"] == g]\n",
    "    plt.scatter(sub[\"vote_share\"], sub[\"ad_share\"], label=g, s=80)\n",
    "\n",
    "# Diagonale y=x\n",
    "low = min(merged[\"vote_share\"].min(), merged[\"ad_share\"].min())\n",
    "high = max(merged[\"vote_share\"].max(), merged[\"ad_share\"].max())\n",
    "plt.plot([low, high], [low, high], linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Vote share DIE LINKE (elections)\")\n",
    "plt.ylabel(\"Share of ad impressions (ads)\")\n",
    "plt.title(\"DIE LINKE: Share of votes vs. ad impressions by age × gender\")\n",
    "plt.legend(title=\"gender\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: print the table\n",
    "display_cols = [\"age_group\",\"gender\",\"vote_share\",\"ad_share\",\"Die Linke\",\"Summe\"]\n",
    "print(merged[display_cols].sort_values([\"gender\",\"age_group\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13baaa23",
   "metadata": {},
   "source": [
    "Robustness Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e4a824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def load_state_votes_from_results_robust(path: str) -> pd.DataFrame:\n",
    "    # Read raw lines and identify header rows\n",
    "    with open(path, \"r\", encoding=\"latin-1\") as f:\n",
    "        lines = [ln.rstrip(\"\\n\") for ln in f.readlines()]\n",
    "\n",
    "    # Assume 3 header rows at 5–7\n",
    "    h1i, h2i, h3i = 5, 6, 7\n",
    "    hdr1 = lines[h1i].split(\";\")\n",
    "    hdr2 = lines[h2i].split(\";\")\n",
    "    hdr3 = lines[h3i].split(\";\")\n",
    "    maxlen = max(len(hdr1), len(hdr2), len(hdr3))\n",
    "    def pad(row): return row + [\"\"]*(maxlen - len(row))\n",
    "    hdr1, hdr2, hdr3 = pad(hdr1), pad(hdr2), pad(hdr3)\n",
    "\n",
    "    data_start = h3i + 1\n",
    "    df = pd.read_csv(path, sep=\";\", header=None, skiprows=data_start,\n",
    "                     encoding=\"latin-1\", dtype=str)\n",
    "    df = df.dropna(how=\"all\")\n",
    "\n",
    "    def htext(j):\n",
    "        a, b, c = hdr1[j].strip().lower(), hdr2[j].strip().lower(), hdr3[j].strip().lower()\n",
    "        return a, b, c, (a + \" | \" + b + \" | \" + c)\n",
    "\n",
    "    region_idx, belongs_idx = None, None\n",
    "    for j in range(maxlen):\n",
    "        a, b, c, _ = htext(j)\n",
    "        if region_idx is None and any(k in a+b+c for k in [\"gebiet\", \"bundesland\", \"land\"]):\n",
    "            region_idx = j\n",
    "        if belongs_idx is None and any(k in a+b+c for k in [\"gehört\", \"gehoert\", \"geh. zu\", \"gehört zu\"]):\n",
    "            belongs_idx = j\n",
    "    if region_idx is None: region_idx = 1\n",
    "    if belongs_idx is None: belongs_idx = 2\n",
    "\n",
    "    def is_zweit_count(j):\n",
    "        a, b, c, _ = htext(j)\n",
    "        txt = a + \" \" + b + \" \" + c\n",
    "        if \"zweit\" not in txt:\n",
    "            return False\n",
    "        if any(k in txt for k in [\"anteil\", \"quote\", \"%\", \"prozent\"]):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    party_cols = {}\n",
    "    tags = {\n",
    "        \"DIE LINKE\": [\"die linke\", \" linke \"],\n",
    "        \"SPD\": [\"spd\"], \"CDU\": [\"cdu\"], \"GRUENE\": [\"grüne\",\"gruene\",\"bündnis 90\",\"buendnis 90\"],\n",
    "        \"FDP\": [\"fdp\"], \"AfD\": [\"afd\"], \"CSU\": [\"csu\"],\n",
    "        \"BSW\": [\"bsw\"], \"FW\": [\"freie wähler\",\"freie waehler\",\"fw\"]\n",
    "    }\n",
    "    for j in range(maxlen):\n",
    "        if not is_zweit_count(j): continue\n",
    "        _, _, _, full = htext(j)\n",
    "        for party, needles in tags.items():\n",
    "            if any(n in full for n in needles):\n",
    "                party_cols.setdefault(party, []).append(j)\n",
    "\n",
    "    def pick_best(idxs):\n",
    "        if not idxs: return None\n",
    "        ranked = []\n",
    "        for j in idxs:\n",
    "            _, _, _, full = htext(j)\n",
    "            score = 0\n",
    "            if \"endg\" in full: score += 2\n",
    "            if \"vorl\" in full: score += 1\n",
    "            ranked.append((score, j))\n",
    "        ranked.sort(reverse=True)\n",
    "        return ranked[0][1]\n",
    "\n",
    "    party_first_idx = {p: pick_best(idxs) for p, idxs in party_cols.items() if idxs}\n",
    "    if party_first_idx.get(\"DIE LINKE\") is None:\n",
    "        suspects = [(j, htext(j)[3]) for j in range(maxlen) if \"zweit\" in htext(j)[3] and \"linke\" in htext(j)[3]]\n",
    "        raise KeyError(\"Could not locate DIE LINKE Zweitstimmen. Candidates:\\n\" +\n",
    "                       \"\\n\".join([f\"[{j}] {txt}\" for j, txt in suspects]))\n",
    "\n",
    "    region = df.iloc[:, region_idx].astype(str).str.strip()\n",
    "    belongs = df.iloc[:, belongs_idx].astype(str).str.strip()\n",
    "    die_linke = pd.to_numeric(df.iloc[:, party_first_idx[\"DIE LINKE\"]], errors=\"coerce\")\n",
    "\n",
    "    total_arrays = []\n",
    "    for party, j in party_first_idx.items():\n",
    "        if j is None: continue\n",
    "        s = pd.to_numeric(df.iloc[:, j], errors=\"coerce\")\n",
    "        total_arrays.append(s)\n",
    "    votes_total = sum(total_arrays) if total_arrays else die_linke.copy()\n",
    "\n",
    "    mask_state = (belongs == \"99\")\n",
    "    out = pd.DataFrame({\n",
    "        \"Region\": region[mask_state],\n",
    "        \"Votes_DieLinke\": die_linke[mask_state],\n",
    "        \"Votes_Total\": votes_total[mask_state],\n",
    "    }).dropna()\n",
    "\n",
    "    out = out.groupby(\"Region\", as_index=False).sum(numeric_only=True)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0b8e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Spending data (already created by your earlier function) ---\n",
    "spending_df = pd.read_csv(\n",
    "    \"data_for_vizzes/spending_by_state/estimated_spending_by_state_die_linke.csv\"\n",
    ").rename(columns={\"State\": \"Region\"})\n",
    "\n",
    "# --- Votes from results_2025_German_election.csv ---\n",
    "votes_state = load_state_votes_from_results_robust(\n",
    "    \"/Users/karlahootz/Desktop/Jura/LLM/Bocconi-Kings College/Material Kings/Dissertation/case study/results_2025_German_election.csv\"\n",
    ")\n",
    "votes_state[\"Vote_Share\"] = votes_state[\"Votes_DieLinke\"] / votes_state[\"Votes_Total\"]\n",
    "\n",
    "# --- Merge ---\n",
    "merged_vote_share = spending_df.merge(\n",
    "    votes_state[[\"Region\", \"Vote_Share\"]], on=\"Region\", how=\"inner\"\n",
    ")\n",
    "\n",
    "# --- Plot + correlation ---\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = merged_vote_share[\"Estimated_Spend\"].astype(float).values\n",
    "y = merged_vote_share[\"Vote_Share\"].astype(float).values\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.scatter(x, y, s=80)\n",
    "m, b = np.polyfit(x, y, 1)\n",
    "plt.plot(np.linspace(min(x), max(x), 100), m*np.linspace(min(x), max(x), 100)+b, linestyle=\"--\")\n",
    "for xi, yi, lab in zip(x, y, merged_vote_share[\"Region\"]):\n",
    "    plt.text(xi, yi, lab, fontsize=8)\n",
    "plt.xlabel(\"Estimated Meta Ad Spend (EUR)\")\n",
    "plt.ylabel(\"Die Linke Vote Share (Zweitstimme)\")\n",
    "plt.title(\"Spending vs Vote Share (Bundesländer)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Pearson r:\", pearsonr(x,y))\n",
    "print(\"Spearman ρ:\", spearmanr(x,y))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}