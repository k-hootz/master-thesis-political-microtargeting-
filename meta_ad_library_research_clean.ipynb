{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start here — paste your file paths (no coding needed)\n",
    "\n",
    "1. Download the two CSV files to your computer:\n",
    "   - **Meta Ad Library export** → e.g. `meta_ad_library.csv`\n",
    "   - **Official election results** → e.g. `election_results.csv`\n",
    "2. Copy the *full path* of each file from your file manager.\n",
    "3. Paste them into the two boxes below (between the quotes) and run the cell (Shift+Enter)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ←← Paste your file paths here\n",
    "META_ADS_PATH = r\"\"  # e.g., r\"C:\\Users\\You\\Downloads\\meta_ad_library.csv\" (Windows) or \"/Users/you/Downloads/meta_ad_library.csv\" (macOS)\n",
    "ELECTIONS_PATH = r\"\"  # e.g., r\"C:\\Users\\You\\Downloads\\election_results.csv\" or \"/Users/you/Downloads/election_results.csv\"\n",
    "\n",
    "def _require_csv(path_str: str, label: str) -> Path:\n",
    "    if not path_str:\n",
    "        raise ValueError(f\"No path provided for {label}. Please paste the full path between the quotes above.\")\n",
    "    p = Path(path_str)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"{label} not found at:\\n{p}\\nPlease check the path.\")\n",
    "    if p.is_dir():\n",
    "        raise IsADirectoryError(f\"{label}: this is a folder, not a CSV file:\\n{p}\")\n",
    "    return p\n",
    "\n",
    "META_ADS_CSV = _require_csv(META_ADS_PATH, \"Meta Ad Library CSV\")\n",
    "ELECTIONS_CSV = _require_csv(ELECTIONS_PATH, \"Election Results CSV\")\n",
    "print(\"Using:\\n  Meta Ad Library:\", META_ADS_CSV, \"\\n  Election Results:\", ELECTIONS_CSV)\n",
    "\n",
    "# Outputs directory (where the notebook will save generated files)\n",
    "from pathlib import Path as _P\n",
    "OUT_DIR = _P(\"outputs\"); OUT_DIR.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note on saved files\n",
    "\n",
    "Figures and tables are displayed directly here in the notebook.\n",
    "\n",
    "At the same time, CSV files and plots are also saved into the folder **`outputs/`**,\n",
    "which will appear in the same location where you saved this notebook.\n",
    "\n",
    "You can open these CSV files in Excel if you want to explore them further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === USER INPUT REQUIRED ===\n",
    "# Please copy/paste the path or URL of the CSV files here\n",
    "\n",
    "META_ADS_CSV   = str(META_ADS_CSV)\n",
    "ELECTIONS_CSV  = str(ELECTIONS_CSV)\n",
    "\n",
    "# Convert to Path objects if they are local files\n",
    "META_ADS_CSV   = Path(META_ADS_CSV)\n",
    "ELECTIONS_CSV  = Path(ELECTIONS_CSV)\n",
    "\n",
    "print(\"Meta ads file:\", META_ADS_CSV)\n",
    "print(\"Election file:\", ELECTIONS_CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e35a6e-80d4-440a-ab7a-838d6ac4c868",
   "metadata": {
    "id": "f6e35a6e-80d4-440a-ab7a-838d6ac4c868"
   },
   "source": [
    "# Meta Ad Library Analysis #\n",
    "\n",
    "This notebook shows code used by Bellingcat to clean and analyse data from the Meta Ad Library from pages running ads sponsored by Die Linke and other affiliated pages seen by Facebook and Instagram users between January 1 and Feb. 23, 2025 in the lead up to the federal election 2025.\n",
    "\n",
    "This code uses the following CSV files:\n",
    "\n",
    "meta-ad-library.csv contains combined data from these advertisers/pages that ran ads sponsored by Die Linke' campaign.\n",
    "\n",
    "results_2025_German_election.csv containing the overall results from all areas (federal territory, federal states, and constituencies) in tabular form published by the Federal Returning Officer.\n",
    "\n",
    "It can also be repurposed to analyse other CSV files downloaded directly from the Meta Ad Library, as long as references to these files are added to data_files.\n",
    "\n",
    "This code was originally put together by Pooja Chaudhuri and Melissa Zhu, but is adjusted by Karla Hootz for the purpose of analysing the rise of Die Linke in the federal election 2025 in Germany."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d33f3a-4e9f-47bb-9fed-666322e1b7f8",
   "metadata": {
    "id": "03d33f3a-4e9f-47bb-9fed-666322e1b7f8"
   },
   "source": [
    "## Imports and basic functions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f040ec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25d2b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2191553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650b76c8-58fe-4ad2-a2f6-58e56b84b42b",
   "metadata": {
    "id": "650b76c8-58fe-4ad2-a2f6-58e56b84b42b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import unicodedata\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d69cb2-3a89-44a0-9c3f-bf7e870bb29e",
   "metadata": {
    "id": "a4d69cb2-3a89-44a0-9c3f-bf7e870bb29e"
   },
   "outputs": [],
   "source": [
    "# Define variables for each candidate's data; replace with appropriate filenames if replicating with other data\n",
    "\n",
    "data_files = {\n",
    "    'die_linke': str(META_ADS_CSV),\n",
    "    # add or replace as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1414c8-1dda-40e4-bcf4-17443a102ed0",
   "metadata": {
    "id": "2f1414c8-1dda-40e4-bcf4-17443a102ed0"
   },
   "outputs": [],
   "source": [
    "def add_average_spending(df):\n",
    "    '''Splits 'spend' column into its upper and lower limits, and calculates the average, adding a new column with this value'''\n",
    "\n",
    "    df[['lower_spend', 'upper_spend']] = df['spend'].str.extract(r'lower_bound:\\s*(\\d+),\\s*upper_bound:\\s*(\\d+)')\n",
    "\n",
    "    df['lower_spend'] = pd.to_numeric(df['lower_spend'])\n",
    "    df['upper_spend'] = pd.to_numeric(df['upper_spend'])\n",
    "\n",
    "    df['spend_average'] = (df['lower_spend'] + df['upper_spend']) / 2\n",
    "\n",
    "    print(f\"The total amount spent on all Meta ads in the data is about ${df['spend_average'].sum()} based on the average of the upper and lower limits for spending on each ad.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951bea54-0f1e-482c-99fd-c0904a9f9ac7",
   "metadata": {
    "id": "951bea54-0f1e-482c-99fd-c0904a9f9ac7"
   },
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "def process_delivery_by_region(df):\n",
    "    '''Takes the data in 'delivery_by_region', which is a dictionary showing the percentage of impressions from each region, and creates\n",
    "    a new column for each region, with the percentage of impressions for that region as the value.'''\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # Parse delivery_by_region and construct a list of rows with region and percentage data\n",
    "    for index, cell in df[['ad_archive_id', 'delivery_by_region']].dropna().iterrows():\n",
    "        try:\n",
    "            # Parse JSON-like data from the cell\n",
    "            parsed_data = json.loads(f'[{cell[\"delivery_by_region\"]}]')\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing cell: {cell['delivery_by_region']}\\n{e}\")\n",
    "            parsed_data = []\n",
    "\n",
    "        # Add ad_archive_id to each parsed entry\n",
    "        for entry in parsed_data:\n",
    "            entry['ad_archive_id'] = cell['ad_archive_id']\n",
    "            rows.append(entry)\n",
    "\n",
    "    # Create a DataFrame from the rows\n",
    "    regions_df = pd.DataFrame(rows)\n",
    "\n",
    "    # Pivot the regions DataFrame to have regions as columns\n",
    "    regions_pivot = regions_df.pivot_table(\n",
    "        index='ad_archive_id',\n",
    "        columns='region',\n",
    "        values='percentage',\n",
    "        aggfunc='first'\n",
    "    ).fillna(0) * 100  # Convert to percentage\n",
    "\n",
    "    # Clean up column names and reset index\n",
    "    regions_pivot.columns.name = None\n",
    "    regions_pivot = regions_pivot.reset_index()\n",
    "\n",
    "    # Merge the pivoted regions data back into the original DataFrame\n",
    "    df_merged = df.merge(regions_pivot, on='ad_archive_id', how='left')\n",
    "\n",
    "    # Remove rows with null delivery_by_region values\n",
    "    df_merged = df_merged.dropna(subset=['delivery_by_region'])\n",
    "\n",
    "    state_columns = ['Baden-W\\u00fcrttemberg', 'Bayern', 'Berlin', 'Brandenburg', 'Bremen',\n",
    "       'Hamburg', 'Hessen', 'Mecklenburg-Vorpommern', 'Niedersachsen', 'Nordrhein-Westfalen', 'Rheinland-Pfalz',\n",
    "       'Sachsen', 'Saxony-Anhalt', 'Schleswig-Holstein', 'Th\\u00fcringen', 'Saarland', 'Berlin (city)', 'Bremen (city)', 'Alsace']\n",
    "\n",
    "    # Print region names\n",
    "    region_names = [col for col in regions_pivot.columns if col != 'ad_archive_id']\n",
    "    non_de_regions = [col for col in region_names if col not in state_columns]\n",
    "    print(\"Region columns added:\")\n",
    "    print(region_names)\n",
    "    print()\n",
    "    print(\"This includes the following non-US regions:\")\n",
    "    print(non_de_regions)\n",
    "    print()\n",
    "\n",
    "    # Calculate the total non-US delivery percentage out of the overall total\n",
    "    non_de_total_percentage = df_merged[non_de_regions].sum().sum() / df_merged[region_names].sum().sum() * 100\n",
    "    print(f\"Total percentage of ad delivery in non-US regions across all ads: {non_de_total_percentage:.3f}%\")\n",
    "\n",
    "    return region_names, non_de_regions, df_merged\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(str(META_ADS_CSV))\n",
    "# Optional: call and visualization\n",
    "region_names, non_de_regions, df_with_regions = process_delivery_by_region(df)\n",
    "\n",
    "# Beispiel-Plot: Durchschnittliche Anteile der Top 10 Regionen\n",
    "region_means = df_with_regions[region_names].mean().sort_values(ascending=False).head(10)\n",
    "region_means.plot(kind=\"barh\")\n",
    "plt.xlabel(\"percentage\")\n",
    "plt.ylabel(\"region\")\n",
    "plt.title(\"Top 10 regional distribution by ad\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48526e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# --- Mapping (from our earlier step) ---\n",
    "city_to_state = {\n",
    "    \"Berlin\": \"Berlin\",\n",
    "    \"Hamburg\": \"Hamburg\",\n",
    "    \"Bielefeld\": \"Nordrhein-Westfalen\",\n",
    "    \"Dresden\": \"Sachsen\",\n",
    "    \"Hildesheim\": \"Niedersachsen\",\n",
    "    \"Erfurt\": \"Thüringen\",\n",
    "    \"Offenbach\": \"Hessen\",\n",
    "    \"Leipzig\": \"Sachsen\",\n",
    "}\n",
    "\n",
    "# Regex patterns to detect city names in text\n",
    "city_patterns = {city: re.compile(rf\"\\b{city}\\b\", re.IGNORECASE) for city in city_to_state.keys()}\n",
    "\n",
    "# --- Add boolean flags for city mentions in ad_creative_bodies ---\n",
    "for city, pattern in city_patterns.items():\n",
    "    df_with_regions[f\"mention_{city}\"] = df_with_regions[\"ad_creative_bodies\"].astype(str).str.contains(pattern)\n",
    "\n",
    "# --- Analysis: compare delivery in matching Bundesland ---\n",
    "results = []\n",
    "\n",
    "for city, state in city_to_state.items():\n",
    "    mention_mask = df_with_regions[f\"mention_{city}\"]\n",
    "\n",
    "    if state in df_with_regions.columns:\n",
    "        avg_with = df_with_regions.loc[mention_mask, state].mean()\n",
    "        avg_without = df_with_regions.loc[~mention_mask, state].mean()\n",
    "        results.append({\n",
    "            \"City\": city,\n",
    "            \"State\": state,\n",
    "            \"Mentions\": mention_mask.sum(),\n",
    "            \"AvgDelivery_withMention\": avg_with,\n",
    "            \"AvgDelivery_withoutMention\": avg_without,\n",
    "            \"Difference\": avg_with - avg_without\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"Difference\", ascending=False)\n",
    "\n",
    "print(\"Correlation between city mentions in text and delivery by region:\")\n",
    "print(results_df)\n",
    "\n",
    "# --- Optional Plot ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(results_df[\"City\"], results_df[\"Difference\"], color=\"steelblue\")\n",
    "plt.axvline(0, color=\"black\", linestyle=\"--\")\n",
    "plt.xlabel(\"Difference in average share (%)\")\n",
    "plt.title(\"Greater reach in the region when the city is mentioned in the ad text\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f2d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Define pattern for Berlin (include district mentions if needed)\n",
    "berlin_pattern = re.compile(r\"\\bberlin\\b|\\bneukölln\\b\", flags=re.IGNORECASE)\n",
    "\n",
    "# Create binary indicator: does the ad mention Berlin?\n",
    "df_with_regions[\"mention_berlin\"] = df_with_regions[\"ad_creative_bodies\"].astype(str).str.contains(berlin_pattern)\n",
    "\n",
    "# Build DataFrame for plotting\n",
    "df_plot = pd.DataFrame({\n",
    "    \"Delivery %\": pd.to_numeric(df_with_regions[\"Berlin\"], errors=\"coerce\"),\n",
    "    \"Mention Berlin\": df_with_regions[\"mention_berlin\"].map({True: \"Yes\", False: \"No\"})\n",
    "}).dropna()\n",
    "\n",
    "# Plot boxplot\n",
    "plt.figure(figsize=(6,4))\n",
    "df_plot.boxplot(column=\"Delivery %\", by=\"Mention Berlin\", grid=False)\n",
    "plt.suptitle(\"\")\n",
    "plt.title(\"Berlin delivery share by text mention\")\n",
    "plt.ylabel(\"Delivery (%)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5bf283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build df_demographics from 'demographic_distribution' ---\n",
    "def safe_json_loads(s):\n",
    "    if not isinstance(s, str):\n",
    "        return []\n",
    "    s = s.replace(\"'\", '\"')\n",
    "    if not s.strip().startswith(\"[\"):\n",
    "        s = \"[\" + s + \"]\"\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "df_with_regions[\"demographic_distribution\"] = df_with_regions[\"demographic_distribution\"].apply(safe_json_loads)\n",
    "\n",
    "df_exploded = df_with_regions.explode(\"demographic_distribution\")\n",
    "\n",
    "df_demographics = pd.concat(\n",
    "    [\n",
    "        df_exploded[[\"ad_archive_id\"]].reset_index(drop=True),\n",
    "        pd.json_normalize(df_exploded[\"demographic_distribution\"]).reset_index(drop=True)\n",
    "    ],\n",
    "    axis=1\n",
    ").dropna(subset=[\"ad_archive_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8e8a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= Microtargeting Index (uses SAME notebook definitions) =================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Utilities\n",
    "def _safe_minmax(x: pd.Series) -> pd.Series:\n",
    "    x = x.astype(float)\n",
    "    if x.max() == x.min():\n",
    "        return pd.Series(0.0, index=x.index)\n",
    "    return (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "def _hhi(shares) -> float:\n",
    "    \"\"\"Herfindahl over shares (expects non-negative; normalizes to sum=1).\"\"\"\n",
    "    arr = np.asarray(shares, dtype=float)\n",
    "    arr = arr[np.isfinite(arr)]\n",
    "    arr = arr[arr >= 0]\n",
    "    s = arr.sum()\n",
    "    if s <= 0:\n",
    "        return 0.0\n",
    "    p = arr / s\n",
    "    return float(np.sum(p * p))\n",
    "\n",
    "# Local terms kept consistent with earlier text analysis\n",
    "GERMAN_STATES = [\n",
    "    \"Baden-Wurttemberg\", \"Bavaria\", \"Berlin\", \"Brandenburg\", \"Bremen\", \"Hamburg\", \"Hesse\",\n",
    "    \"Mecklenburg-Western Pomerania\", \"Lower Saxony\", \"North Rhine-Westphalia\", \"Rhineland-Palatinate\",\n",
    "    \"Saarland\", \"Saxony\", \"Saxony-Anhalt\", \"Schleswig-Holstein\", \"Thuringia\"\n",
    "]\n",
    "CITIES_CORE = [\"Berlin\", \"Hamburg\", \"Bielefeld\", \"Dresden\", \"Hildesheim\", \"Erfurt\", \"Offenbach\", \"Leipzig\"]\n",
    "DISTRICT_TO_CITY = {\"Neukoelln\": \"Berlin\"}  # extend as needed\n",
    "\n",
    "LOCAL_TERMS = set(GERMAN_STATES) | set(CITIES_CORE) | set(DISTRICT_TO_CITY.keys())\n",
    "TERM_REGEX = {t: re.compile(r\"(?<![\\wÄÖÜäöüß])\" + re.escape(t) + r\"(?![\\wÄÖÜäöüß])\", re.IGNORECASE)\n",
    "              for t in LOCAL_TERMS}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ac5c5-75b7-4f0a-8ca5-f548dba09321",
   "metadata": {
    "id": "e95ac5c5-75b7-4f0a-8ca5-f548dba09321"
   },
   "source": [
    "## Describing the data and processing it for further analysis ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca255d4-fdfb-46ec-8250-785c43e68f81",
   "metadata": {
    "id": "4ca255d4-fdfb-46ec-8250-785c43e68f81"
   },
   "outputs": [],
   "source": [
    "processed_data = {} # initialising empty dictionary to save processed DataFrames\n",
    "region_names_dict = {} # to save region names in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3480ae-e3d3-4f7f-9a49-40f99ab486f9",
   "metadata": {
    "id": "5c3480ae-e3d3-4f7f-9a49-40f99ab486f9",
    "outputId": "0dbfbc1a-00a2-4526-b4bd-61530bce572b"
   },
   "outputs": [],
   "source": [
    "# Run the same functions on all files in the data_files\n",
    "\n",
    "for name, file in data_files.items():\n",
    "\n",
    "    print(f\"### PROCESSING META AD DATA FOR {name.upper()} ###\")\n",
    "    print()\n",
    "\n",
    "    df = pd.read_csv(file) # read the data file as a DataFrame\n",
    "\n",
    "    print(f\"This dataset contains {len(df)} ads from {len(df['page_name'].unique())} pages.\")\n",
    "    print()\n",
    "    print(\"The pages included are:\")\n",
    "    print()\n",
    "    print(df['page_name'].unique())\n",
    "    print()\n",
    "\n",
    "    df = add_average_spending(df)\n",
    "    print()\n",
    "\n",
    "    region_names, df_merged = process_delivery_by_region(df)\n",
    "    print()\n",
    "\n",
    "    region_names_dict[name] = region_names\n",
    "\n",
    "    processed_data[name] = df_merged\n",
    "\n",
    "    print(f\"DataFrame saved as processed_data['{name}'].\")\n",
    "    print(f\"Names of region columns added saved as region_names_dict['{name}'].\")\n",
    "    print()\n",
    "\n",
    "    print(\"-------------------------------------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981ecf74-16e2-4640-a838-a82bda9c72c9",
   "metadata": {
    "id": "981ecf74-16e2-4640-a838-a82bda9c72c9"
   },
   "source": [
    "## Further analysis ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74309a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define a single place for all generated artifacts\n",
    "OUT_DIR = Path(\"outputs\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def get_spending_by_platform(name: str, df: pd.DataFrame, outdir: Path = OUT_DIR):\n",
    "    \"\"\"\n",
    "    Calculate the total spend by platform and export a CSV.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        A short label for the campaign (used in the filename).\n",
    "    df : pd.DataFrame\n",
    "        Must contain columns: 'spend_average' and 'publisher_platforms'.\n",
    "    outdir : Path\n",
    "        Base directory for outputs (default: 'outputs').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    spending_df : pd.DataFrame\n",
    "    out_path : Path\n",
    "    \"\"\"\n",
    "    # Basic checks\n",
    "    required = {\"spend_average\", \"publisher_platforms\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing columns in df: {missing}\")\n",
    "\n",
    "    # Filter by platform\n",
    "    df_fb_insta = df[df[\"publisher_platforms\"] == \"facebook,instagram\"]\n",
    "    df_fb       = df[df[\"publisher_platforms\"] == \"facebook\"]\n",
    "    df_insta    = df[df[\"publisher_platforms\"] == \"instagram\"]\n",
    "\n",
    "    # Spending per platform\n",
    "    fb_insta_spend = df_fb_insta[\"spend_average\"].sum()\n",
    "    fb_spend       = df_fb[\"spend_average\"].sum()\n",
    "    insta_spend    = df_insta[\"spend_average\"].sum()\n",
    "    total_spend    = fb_insta_spend + fb_spend + insta_spend\n",
    "\n",
    "    # Status prints\n",
    "    n = len(df)\n",
    "    pct = lambda x: round(100 * x / n, 2) if n else 0.0\n",
    "    print(f\"{pct(len(df_fb_insta))}% of ads for {name.capitalize()}'s campaign ran on both Facebook and Instagram.\")\n",
    "    print(f\"{pct(len(df_fb))}% only on Facebook, and {pct(len(df_insta))}% only on Instagram.\")\n",
    "    print(f\"This added up to ${total_spend} in total.\\n\")\n",
    "\n",
    "    # DataFrame\n",
    "    spending_df = pd.DataFrame({\n",
    "        \"Platform\": [\"Facebook & Instagram\", \"Facebook\", \"Instagram\", \"Total\"],\n",
    "        \"Total_Spend\": [fb_insta_spend,       fb_spend,   insta_spend, total_spend]\n",
    "    })\n",
    "\n",
    "    # Save under a clean outputs structure\n",
    "    platform_dir = outdir / \"spending_by_platform\"\n",
    "    platform_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = platform_dir / f\"spending_by_platform_{name}.csv\"\n",
    "    spending_df.to_csv(out_path, index=False)\n",
    "    print(f\"Spending data exported to: {out_path}\")\n",
    "\n",
    "    return spending_df, out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741e1ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Estimate spending by state (portable) ====\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# single place for generated artifacts (typically gitignored)\n",
    "OUT_DIR = Path(\"outputs\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def _ensure_spend_average(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Ensure df has numeric 'spend_average'. If absent, derive midpoint from 'spend' ranges.\"\"\"\n",
    "    if \"spend_average\" in df.columns:\n",
    "        return df\n",
    "    if \"spend\" in df.columns:\n",
    "        def _mid(s):\n",
    "            if pd.isna(s):\n",
    "                return 0.0\n",
    "            nums = re.findall(r\"\\d+\", str(s))\n",
    "            if len(nums) >= 2:\n",
    "                lo, hi = map(int, nums[:2])\n",
    "                return (lo + hi) / 2.0\n",
    "            elif len(nums) == 1:\n",
    "                return float(nums[0])\n",
    "            return 0.0\n",
    "        df = df.copy()\n",
    "        df[\"spend_average\"] = df[\"spend\"].apply(_mid)\n",
    "        return df\n",
    "    raise KeyError(\"Neither 'spend_average' nor 'spend' found in the DataFrame.\")\n",
    "\n",
    "def estimate_spending_by_state(\n",
    "    name: str,\n",
    "    df: pd.DataFrame,\n",
    "    outdir: Path = OUT_DIR,\n",
    "    plot: bool = True,\n",
    ") -> tuple[pd.DataFrame, Path]:\n",
    "    \"\"\"\n",
    "    Estimate ad spending per German state by multiplying spend_average by delivery percentages.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        Label for file output (e.g., 'die_linke').\n",
    "    df : pd.DataFrame\n",
    "        Must include spend column(s) and state percentage columns (0..100).\n",
    "    outdir : Path\n",
    "        Output base directory (default: 'outputs').\n",
    "    plot : bool\n",
    "        If True, show a bar chart.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    state_spending_df : pd.DataFrame\n",
    "        Columns: ['State', 'Estimated_Spend'] sorted desc.\n",
    "    out_path : Path\n",
    "        Path to the saved CSV.\n",
    "    \"\"\"\n",
    "    # candidates incl. variants seen in your data\n",
    "    state_candidates = [\n",
    "        \"Baden-Württemberg\",\"Bayern\",\"Berlin\",\"Brandenburg\",\"Bremen\",\"Hamburg\",\"Hessen\",\n",
    "        \"Mecklenburg-Vorpommern\",\"Niedersachsen\",\"Nordrhein-Westfalen\",\"Rheinland-Pfalz\",\n",
    "        \"Saarland\",\"Sachsen\",\"Sachsen-Anhalt\",\"Schleswig-Holstein\",\"Thüringen\",\n",
    "        # extras seen in your dataset\n",
    "        \"Saxony-Anhalt\",\"Berlin (city)\",\"Bremen (city)\",\"Alsace\",\n",
    "    ]\n",
    "\n",
    "    df = _ensure_spend_average(df)\n",
    "\n",
    "    # Keep only the state columns that actually exist\n",
    "    available_states = [s for s in state_candidates if s in df.columns]\n",
    "    if not available_states:\n",
    "        raise ValueError(\n",
    "            \"No matching state columns were found in the DataFrame.\\n\"\n",
    "            \"Expected one or more of:\\n  - \" + \", \".join(state_candidates)\n",
    "        )\n",
    "\n",
    "    # Make sure state columns are numeric (coerce invalid to 0)\n",
    "    df_num = df.copy()\n",
    "    for s in available_states:\n",
    "        df_num[s] = pd.to_numeric(df_num[s], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # Compute estimated spend per state\n",
    "    state_spending = {\n",
    "        s: float((df_num[\"spend_average\"] * (df_num[s] / 100.0)).sum())\n",
    "        for s in available_states\n",
    "    }\n",
    "\n",
    "    state_spending_df = (\n",
    "        pd.DataFrame({\"State\": list(state_spending.keys()),\n",
    "                      \"Estimated_Spend\": list(state_spending.values())})\n",
    "        .sort_values(\"Estimated_Spend\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Save under a clean outputs structure\n",
    "    state_dir = outdir / \"spending_by_state\"\n",
    "    state_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = state_dir / f\"estimated_spending_by_state_{name}.csv\"\n",
    "    state_spending_df.to_csv(out_path, index=False)\n",
    "    print(f\"Saved: {out_path}\")\n",
    "\n",
    "    # Top-3 message\n",
    "    top3 = state_spending_df.head(3)[\"State\"].tolist()\n",
    "    print(f\"{name.capitalize()} spent the most in: {', '.join(top3)}\")\n",
    "\n",
    "    # Plot\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(state_spending_df[\"State\"], state_spending_df[\"Estimated_Spend\"])\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.ylabel(\"Estimated Spend (EUR)\")\n",
    "        plt.title(f\"Estimated Spending by State — {name}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return state_spending_df, out_path\n",
    "\n",
    "# Usage example (if you have df_with_regions already prepared):\n",
    "# result_df, result_csv_path = estimate_spending_by_state(\"die_linke\", df_with_regions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8492e173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Merge + plot: Spending vs Votes (portable) ====\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Where we put generated artifacts by default\n",
    "OUT_DIR = Path(\"outputs\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ---- 1) Get spending-by-state ----\n",
    "# Preferred: use the in-memory result from estimate_spending_by_state(...)\n",
    "if \"spending_by_state_df\" in globals() and isinstance(spending_by_state_df, pd.DataFrame):\n",
    "    _spending = spending_by_state_df.copy()\n",
    "# Or maybe you stored it just as 'spending_df' with State/Estimated_Spend?\n",
    "elif \"spending_df\" in globals() and isinstance(spending_df, pd.DataFrame) and \\\n",
    "     set([\"State\"]).issubset(spending_df.columns):\n",
    "    _spending = spending_df.copy()\n",
    "else:\n",
    "    # Fallback: let the user paste a local CSV path (manual mode)\n",
    "    SPENDING_CSV_PATH = r\"\"  # e.g., r\"C:\\Users\\Prof\\Downloads\\estimated_spending_by_state_die_linke.csv\"\n",
    "    if not SPENDING_CSV_PATH:\n",
    "        raise ValueError(\n",
    "            \"No in-memory spending data found. Please either run the cell that creates \"\n",
    "            \"spending_by_state_df or paste a local path into SPENDING_CSV_PATH.\"\n",
    "        )\n",
    "    _spending = pd.read_csv(Path(SPENDING_CSV_PATH))\n",
    "\n",
    "# Harmonize column names for spending\n",
    "if \"Region\" not in _spending.columns and \"State\" in _spending.columns:\n",
    "    _spending = _spending.rename(columns={\"State\": \"Region\"})\n",
    "if \"Estimated_Spend\" not in _spending.columns:\n",
    "    # Common alternates:\n",
    "    if \"Total_Spend\" in _spending.columns:\n",
    "        _spending = _spending.rename(columns={\"Total_Spend\": \"Estimated_Spend\"})\n",
    "    elif \"Estimated_Spend_EUR\" in _spending.columns:\n",
    "        _spending = _spending.rename(columns={\"Estimated_Spend_EUR\": \"Estimated_Spend\"})\n",
    "    else:\n",
    "        raise KeyError(\n",
    "            \"Spending data is missing 'Estimated_Spend'. Found columns:\\n\"\n",
    "            + \", \".join(_spending.columns)\n",
    "        )\n",
    "\n",
    "# ---- 2) Get votes by state ----\n",
    "# Preferred: use in-memory 'votes_state' from load_state_votes_from_results_robust(...)\n",
    "if \"votes_state\" in globals() and isinstance(votes_state, pd.DataFrame):\n",
    "    _votes = votes_state.copy()\n",
    "    # Build a 'Votes' column compatible with plotting (absolute Die Linke votes)\n",
    "    if \"Votes\" not in _votes.columns:\n",
    "        if \"Votes_DieLinke\" in _votes.columns:\n",
    "            _votes[\"Votes\"] = _votes[\"Votes_DieLinke\"]\n",
    "        elif \"DieLinke_Zweitstimmen_Endgueltig\" in _votes.columns:\n",
    "            _votes = _votes.rename(columns={\"DieLinke_Zweitstimmen_Endgueltig\": \"Votes\"})\n",
    "        else:\n",
    "            raise KeyError(\n",
    "                \"Could not locate a Die Linke votes column in votes_state. \"\n",
    "                \"Expected 'Votes_DieLinke' or 'DieLinke_Zweitstimmen_Endgueltig'.\"\n",
    "            )\n",
    "# Otherwise: manual path to a CSV that has Region + (Votes or DieLinke_Zweitstimmen_Endgueltig)\n",
    "else:\n",
    "    RESULTS_STATE_CSV_PATH = r\"\"  # e.g., r\"C:\\Users\\Prof\\Downloads\\die_linke_regions_2025.csv\"\n",
    "    if not RESULTS_STATE_CSV_PATH:\n",
    "        raise ValueError(\n",
    "            \"No in-memory results found. Please either run the cell that creates 'votes_state' \"\n",
    "            \"or paste a local path into RESULTS_STATE_CSV_PATH.\"\n",
    "        )\n",
    "    _votes = pd.read_csv(Path(RESULTS_STATE_CSV_PATH))\n",
    "\n",
    "    # Harmonize column names for votes\n",
    "    if \"Region\" not in _votes.columns:\n",
    "        # Try a few common variants if needed\n",
    "        for cand in [\"Bundesland\", \"Land\", \"State\"]:\n",
    "            if cand in _votes.columns:\n",
    "                _votes = _votes.rename(columns={cand: \"Region\"})\n",
    "                break\n",
    "    if \"Votes\" not in _votes.columns:\n",
    "        if \"DieLinke_Zweitstimmen_Endgueltig\" in _votes.columns:\n",
    "            _votes = _votes.rename(columns={\"DieLinke_Zweitstimmen_Endgueltig\": \"Votes\"})\n",
    "        elif \"Votes_DieLinke\" in _votes.columns:\n",
    "            _votes = _votes.rename(columns={\"Votes_DieLinke\": \"Votes\"})\n",
    "        else:\n",
    "            raise KeyError(\n",
    "                \"Could not find a Die Linke votes column. Expected one of \"\n",
    "                \"['Votes', 'Votes_DieLinke', 'DieLinke_Zweitstimmen_Endgueltig'].\"\n",
    "            )\n",
    "\n",
    "# Minimal column checks\n",
    "for df_, needed in [(_spending, {\"Region\", \"Estimated_Spend\"}), (_votes, {\"Region\", \"Votes\"})]:\n",
    "    missing = needed - set(df_.columns)\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing columns {missing} in dataframe with columns: {list(df_.columns)}\")\n",
    "\n",
    "# ---- 3) Merge ----\n",
    "merged = pd.merge(_spending[[\"Region\", \"Estimated_Spend\"]],\n",
    "                  _votes[[\"Region\", \"Votes\"]],\n",
    "                  on=\"Region\", how=\"inner\")\n",
    "\n",
    "# If you prefer vote share instead of raw votes:\n",
    "# if \"Votes_DieLinke\" in _votes.columns and \"Votes_Total\" in _votes.columns:\n",
    "#     merged = pd.merge(_spending[[\"Region\", \"Estimated_Spend\"]],\n",
    "#                       (_votes.assign(Vote_Share=_votes[\"Votes_DieLinke\"] / _votes[\"Votes_Total\"])\n",
    "#                              [[\"Region\", \"Vote_Share\"]]),\n",
    "#                       on=\"Region\", how=\"inner\")\n",
    "#     y_col = \"Vote_Share\"\n",
    "# else:\n",
    "y_col = \"Votes\"\n",
    "\n",
    "# ---- 4) Plot ----\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=merged, x=\"Estimated_Spend\", y=y_col, hue=\"Region\", s=100)\n",
    "\n",
    "# Regression line for overall trend (not per-region)\n",
    "sns.regplot(data=merged, x=\"Estimated_Spend\", y=y_col,\n",
    "            scatter=False, color=\"black\", line_kws={\"linestyle\": \"--\"})\n",
    "\n",
    "plt.title(\"Ad Spending vs. Die Linke Votes (2025, by Bundesland)\")\n",
    "plt.xlabel(\"Estimated Spending (€)\")\n",
    "plt.ylabel(\"Die Linke Votes (Zweitstimme)\" if y_col == \"Votes\" else \"Die Linke Vote Share\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- 5) Correlations ----\n",
    "pearson_corr = merged[\"Estimated_Spend\"].corr(merged[y_col], method=\"pearson\")\n",
    "spearman_corr = merged[\"Estimated_Spend\"].corr(merged[y_col], method=\"spearman\")\n",
    "\n",
    "print(\"Correlation between spending and votes/share:\")\n",
    "print(f\"  Pearson r = {pearson_corr:.3f}\")\n",
    "print(f\"  Spearman ρ = {spearman_corr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3e2eb3-dec4-40b3-9405-517dd1681536",
   "metadata": {
    "id": "ca3e2eb3-dec4-40b3-9405-517dd1681536"
   },
   "outputs": [],
   "source": [
    "def drop_empty_ad_text(df):\n",
    "    '''Removes all rows that do not have text in the 'ad_creative_bodies' column, for example video ads, for the purposes of text analysis.'''\n",
    "\n",
    "    df_clean = df.dropna(subset=['ad_creative_bodies'], how='all')\n",
    "    unique_ads_text = df_clean['ad_creative_bodies'].unique()\n",
    "\n",
    "    print(f\"Of the ads in the data, {len(df)-len(df_clean)} did not have any text accompanying the ads – for example video ads.\")\n",
    "    print(f\"For textual analysis, we will only look at the remaining {len(df_clean)} ads that have text, including {len(unique_ads_text)} unique ones.\")\n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22d3884-4e9c-4140-80f4-a2c552a4d906",
   "metadata": {
    "id": "f22d3884-4e9c-4140-80f4-a2c552a4d906"
   },
   "outputs": [],
   "source": [
    "# Initialise dictionary to store word combinations\n",
    "top_bigrams = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1177a95-1685-4621-9623-1f7b40d24b6e",
   "metadata": {
    "id": "f1177a95-1685-4621-9623-1f7b40d24b6e"
   },
   "outputs": [],
   "source": [
    "def normalise_unicode(text):\n",
    "    '''Normalise any stylised Unicode text to standard alphabetic characters'''\n",
    "\n",
    "    normalised_text = ''.join(\n",
    "        unicodedata.normalize('NFKD', c)[0] if 'MATHEMATICAL' in unicodedata.name(c, '') else c\n",
    "        for c in text\n",
    "    )\n",
    "    return normalised_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd15a4b-2ac1-4b2f-add2-fae1d3522f0a",
   "metadata": {
    "id": "ebd15a4b-2ac1-4b2f-add2-fae1d3522f0a"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    '''Remove times in formats like '4:00PM', '12:30AM', etc.'''\n",
    "\n",
    "    text = re.sub(r'\\b\\d{1,2}:\\d{2}\\s?(AM|PM|am|pm)?\\b', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6ec857-1952-4852-a5ae-b42b6cb70f67",
   "metadata": {
    "id": "2c6ec857-1952-4852-a5ae-b42b6cb70f67"
   },
   "outputs": [],
   "source": [
    "# Download stopwords, i.e. common words like 'the'\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973565ff-d7a5-4719-865a-2ae8844a8fdd",
   "metadata": {
    "id": "973565ff-d7a5-4719-865a-2ae8844a8fdd",
    "outputId": "690a834c-c37f-4c20-fde9-ac21f66b3300",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the same functions to analyse all of the data files\n",
    "\n",
    "for name, df in processed_data.items():\n",
    "\n",
    "    print(f\"### ANALYSING META AD DATA FOR {name.upper()} ###\")\n",
    "    print()\n",
    "\n",
    "    spending_df = get_spending_by_platform(name, df)\n",
    "    print()\n",
    "\n",
    "    state_spending_df = estimate_spending_by_state(name, df)\n",
    "    print()\n",
    "\n",
    "    df_clean = drop_empty_ad_text(df)\n",
    "    print()\n",
    "\n",
    "    label_ctas(name, df_clean)\n",
    "    print()\n",
    "\n",
    "    get_most_frequent_phrases(name, df_clean)\n",
    "    print()\n",
    "\n",
    "    print(\"-------------------------------------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946d3706-45b6-48c8-934c-a678b2d5bd9a",
   "metadata": {
    "id": "946d3706-45b6-48c8-934c-a678b2d5bd9a"
   },
   "outputs": [],
   "source": [
    "#demographic_distribution_analysis.py\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "csv_path = str(META_ADS_CSV)\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "def safe_json_loads(s):\n",
    "    if not isinstance(s, str):\n",
    "        return []\n",
    "    # Replace single quotes with double quotes\n",
    "    s = s.replace(\"'\", '\"')\n",
    "    # Add brackets if missing\n",
    "    if not s.strip().startswith(\"[\"):\n",
    "        s = \"[\" + s + \"]\"\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing: {s}\\n{e}\")\n",
    "        return []\n",
    "\n",
    "df[\"demographic_distribution\"] = df[\"demographic_distribution\"].apply(safe_json_loads)\n",
    "\n",
    "df_exploded = df.explode(\"demographic_distribution\")   \n",
    "\n",
    "df_demographics = pd.concat([df_exploded.drop(columns=[\"demographic_distribution\"]).reset_index(drop=True),pd.json_normalize(df_exploded[\"demographic_distribution\"]).reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(df_demographics)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Group by age group and gender and compute the mean\n",
    "pivot = df_demographics.groupby([\"age\", \"gender\"])['percentage'].mean().unstack()\n",
    "\n",
    "# Sort age groups logically (optional, if needed)\n",
    "# age_order = [\"18-24\", \"25-34\", \"35-44\", \"45-54\", \"55-64\", \"65+\"]\n",
    "# pivot = pivot.reindex(age_order)\n",
    "\n",
    "# Balkendiagramm zeichnen\n",
    "pivot.plot(kind=\"barh\")\n",
    "plt.xlabel(\"Average share\")\n",
    "plt.ylabel(\"age group\")\n",
    "plt.title(\"Average demographic distribution by age group and gender\")\n",
    "plt.legend(title=\"gender\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Save the processed DataFrame to a CSV file\n",
    "output_csv_path = \"data_for_vizzes/demographic_distribution.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff574d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship between spend and impressions\n",
    "def extract_midpoint(value):\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    try:\n",
    "        bounds = re.findall(r\"\\d+\", value)\n",
    "        if len(bounds) == 2:\n",
    "            lower = int(bounds[0])\n",
    "            upper = int(bounds[1])\n",
    "            return (lower + upper) / 2\n",
    "        elif len(bounds) == 1:\n",
    "            return int(bounds[0])\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Midpoints berechnen\n",
    "df[\"impressions_mid\"] = df[\"impressions\"].apply(extract_midpoint)\n",
    "df[\"spend_mid\"] = df[\"spend\"].apply(extract_midpoint)\n",
    "\n",
    "# Keep only valid rows\n",
    "valid_rows = df.dropna(subset=[\"impressions_mid\", \"spend_mid\"])\n",
    "\n",
    "# Korrelation berechnen\n",
    "correlation = valid_rows[\"impressions_mid\"].corr(valid_rows[\"spend_mid\"])\n",
    "print(f\"Korrelationskoeffizient zwischen Spend und Impressions: {correlation:.3f}\")\n",
    "\n",
    "# Scatter plot with log–log scale\n",
    "if (valid_rows[\"spend_mid\"] > 0).all() and (valid_rows[\"impressions_mid\"] > 0).all():\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(valid_rows[\"spend_mid\"], valid_rows[\"impressions_mid\"], alpha=0.5)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"Spend (Midpoint EUR, log scale)\")\n",
    "    plt.ylabel(\"Impressions (Midpoint, log scale)\")\n",
    "    plt.title(\"Correlation between spend and impressions\")\n",
    "    plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(valid_rows[\"spend_mid\"], valid_rows[\"impressions_mid\"], alpha=0.5)\n",
    "    plt.xlabel(\"Spend (Midpoint EUR)\")\n",
    "    plt.ylabel(\"Impressions (Midpoint)\")\n",
    "    plt.title(\"Correlation between spend and impressions\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d550b8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse voting data for the party \"Die Linke\" in the 2025 Bundestag elections\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the file (adjust if needed)\n",
    "vote_file = \"/Users/karlahootz/Desktop/Jura/LLM/Bocconi-Kings College/Material Kings/Dissertation/case study/btw25_rws_bst2.csv\"\n",
    "\n",
    "# Read CSV, skip metadata rows, use ';' as delimiter and latin-1 encoding\n",
    "votes_raw = pd.read_csv(vote_file, sep=\";\", skiprows=11, encoding=\"latin-1\")\n",
    "\n",
    "# Recode gender: w -> female, m|d|o -> male_other, Summe -> all\n",
    "votes_raw[\"gender\"] = votes_raw[\"Geschlecht\"].replace({\n",
    "    \"w\": \"female\",\n",
    "    \"m|d|o\": \"male_other\",\n",
    "    \"Summe\": \"all\"\n",
    "})\n",
    "\n",
    "# Keep only female and male_other\n",
    "votes = votes_raw[votes_raw[\"gender\"].isin([\"female\", \"male_other\"])].copy()\n",
    "\n",
    "# Select only relevant columns (age, gender, votes for Die Linke, total)\n",
    "votes = votes[[\"Geburtsjahresgruppe\", \"gender\", \"Die Linke\", \"Summe\"]].rename(\n",
    "    columns={\"Geburtsjahresgruppe\": \"age_group\", \"Die Linke\": \"DIE LINKE\"}\n",
    ")\n",
    "\n",
    "print(votes.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28203fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1) Load election CSV (header after metadata) ---\n",
    "vote_file = \"/Users/karlahootz/Desktop/Jura/LLM/Bocconi-Kings College/Material Kings/Dissertation/case study/btw25_rws_bst2.csv\"\n",
    "\n",
    "votes_raw = pd.read_csv(vote_file, sep=\";\", skiprows=11, encoding=\"latin-1\")\n",
    "\n",
    "# Only second vote (party preference)\n",
    "votes_raw = votes_raw[votes_raw[\"Erst-/Zweitstimme\"] == 2].copy()\n",
    "\n",
    "# Geschlecht harmonisieren\n",
    "votes_raw[\"gender\"] = votes_raw[\"Geschlecht\"].replace({\n",
    "    \"w\": \"female\",\n",
    "    \"m|d|o\": \"male_other\",\n",
    "    \"Summe\": \"all\"\n",
    "})\n",
    "\n",
    "# Age cohorts -> ad age buckets\n",
    "age_map = {\n",
    "    \"2001-2007\": \"18-24\",\n",
    "    \"1991-2000\": \"25-34\",\n",
    "    \"1981-1990\": \"35-44\",\n",
    "    \"1966-1980\": \"45-54\",\n",
    "    \"1956-1965\": \"55-64\",\n",
    "    \"<=1955\": \"65+\",\n",
    "    \"≤1955\": \"65+\"  # manche Dateien nutzen das ≤-Zeichen\n",
    "}\n",
    "votes_raw[\"age_group\"] = votes_raw[\"Geburtsjahresgruppe\"].map(age_map)\n",
    "\n",
    "# Nur weiblich & male_other behalten\n",
    "votes = votes_raw[votes_raw[\"gender\"].isin([\"female\",\"male_other\"])].copy()\n",
    "\n",
    "# Stimmenanteil DIE LINKE je Alters×Geschlecht\n",
    "# Note: column name must match your file exactly (\"Die Linke\")\n",
    "if \"Die Linke\" not in votes.columns:\n",
    "    raise KeyError(\"Spalte 'Die Linke' nicht gefunden. Prüfe die genaue Schreibweise der Parteispalte.\")\n",
    "\n",
    "votes_grp = (\n",
    "    votes.groupby([\"age_group\",\"gender\"], dropna=False)[[\"Die Linke\",\"Summe\"]]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "votes_grp = votes_grp.dropna(subset=[\"age_group\"])  # nur gemappte Altersgruppen\n",
    "votes_grp[\"vote_share\"] = votes_grp[\"Die Linke\"] / votes_grp[\"Summe\"]\n",
    "\n",
    "# --- 2) Map ad impression shares to the same groups ---\n",
    "# Expects df_demographics with columns: age (e.g., '18-24'), gender ('female','male','unknown'), percentage (0..1)\n",
    "ads_long = df_demographics.copy()\n",
    "\n",
    "# weiblich direkt\n",
    "ads_female = (\n",
    "    ads_long[ads_long[\"gender\"] == \"female\"]\n",
    "    .groupby(\"age\", as_index=False)[\"percentage\"]\n",
    "    .mean()\n",
    "    .rename(columns={\"age\":\"age_group\", \"percentage\":\"ad_share_female\"})\n",
    ")\n",
    "\n",
    "# male_other = male + unknown\n",
    "ads_male = (\n",
    "    ads_long[ads_long[\"gender\"] == \"male\"]\n",
    "    .groupby(\"age\", as_index=False)[\"percentage\"]\n",
    "    .mean()\n",
    "    .rename(columns={\"age\":\"age_group\", \"percentage\":\"ad_share_male\"})\n",
    ")\n",
    "ads_unknown = (\n",
    "    ads_long[ads_long[\"gender\"] == \"unknown\"]\n",
    "    .groupby(\"age\", as_index=False)[\"percentage\"]\n",
    "    .mean()\n",
    "    .rename(columns={\"age\":\"age_group\", \"percentage\":\"ad_share_unknown\"})\n",
    ")\n",
    "\n",
    "ads_comb = (\n",
    "    ads_female\n",
    "    .merge(ads_male, on=\"age_group\", how=\"outer\")\n",
    "    .merge(ads_unknown, on=\"age_group\", how=\"outer\")\n",
    "    .fillna(0.0)\n",
    ")\n",
    "ads_comb[\"ad_share_male_other\"] = ads_comb[\"ad_share_male\"] + ads_comb[\"ad_share_unknown\"]\n",
    "\n",
    "# in Long-Form bringen (age_group, gender, ad_share)\n",
    "ads_long2 = pd.concat([\n",
    "    ads_comb[[\"age_group\",\"ad_share_female\"]].assign(gender=\"female\").rename(columns={\"ad_share_female\":\"ad_share\"}),\n",
    "    ads_comb[[\"age_group\",\"ad_share_male_other\"]].assign(gender=\"male_other\").rename(columns={\"ad_share_male_other\":\"ad_share\"}),\n",
    "], ignore_index=True)\n",
    "\n",
    "# --- 3) Merge: Votes vs Ads ---\n",
    "merged = votes_grp.merge(ads_long2, on=[\"age_group\",\"gender\"], how=\"inner\")\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# Drop NaNs just in case\n",
    "df_corr = merged.dropna(subset=[\"vote_share\", \"ad_share\"])\n",
    "\n",
    "# Pearson correlation (linear)\n",
    "pearson_r, pearson_p = pearsonr(df_corr[\"vote_share\"], df_corr[\"ad_share\"])\n",
    "\n",
    "# Spearman correlation (rank-based, monotonic)\n",
    "spearman_r, spearman_p = spearmanr(df_corr[\"vote_share\"], df_corr[\"ad_share\"])\n",
    "\n",
    "print(\"Correlation between vote share (Die Linke) and ad impressions:\")\n",
    "print(f\" Pearson r = {pearson_r:.3f} (p={pearson_p:.4f})\")\n",
    "print(f\" Spearman ρ = {spearman_r:.3f} (p={spearman_p:.4f})\")\n",
    "\n",
    "\n",
    "# --- 4) Scatterplot ---\n",
    "plt.figure(figsize=(7,7))\n",
    "for g in merged[\"gender\"].unique():\n",
    "    sub = merged[merged[\"gender\"] == g]\n",
    "    plt.scatter(sub[\"vote_share\"], sub[\"ad_share\"], label=g, s=80)\n",
    "\n",
    "# Diagonale y=x\n",
    "low = min(merged[\"vote_share\"].min(), merged[\"ad_share\"].min())\n",
    "high = max(merged[\"vote_share\"].max(), merged[\"ad_share\"].max())\n",
    "plt.plot([low, high], [low, high], linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Vote share DIE LINKE (elections)\")\n",
    "plt.ylabel(\"Share of ad impressions (ads)\")\n",
    "plt.title(\"DIE LINKE: Share of votes vs. ad impressions by age × gender\")\n",
    "plt.legend(title=\"gender\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: print the table\n",
    "display_cols = [\"age_group\",\"gender\",\"vote_share\",\"ad_share\",\"Die Linke\",\"Summe\"]\n",
    "print(merged[display_cols].sort_values([\"gender\",\"age_group\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13baaa23",
   "metadata": {},
   "source": [
    "Robustness Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e4a824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def load_state_votes_from_results_robust(path: str) -> pd.DataFrame:\n",
    "    # Read raw lines and identify header rows\n",
    "    with open(path, \"r\", encoding=\"latin-1\") as f:\n",
    "        lines = [ln.rstrip(\"\\n\") for ln in f.readlines()]\n",
    "\n",
    "    # Assume 3 header rows at 5–7\n",
    "    h1i, h2i, h3i = 5, 6, 7\n",
    "    hdr1 = lines[h1i].split(\";\")\n",
    "    hdr2 = lines[h2i].split(\";\")\n",
    "    hdr3 = lines[h3i].split(\";\")\n",
    "    maxlen = max(len(hdr1), len(hdr2), len(hdr3))\n",
    "    def pad(row): return row + [\"\"]*(maxlen - len(row))\n",
    "    hdr1, hdr2, hdr3 = pad(hdr1), pad(hdr2), pad(hdr3)\n",
    "\n",
    "    data_start = h3i + 1\n",
    "    df = pd.read_csv(path, sep=\";\", header=None, skiprows=data_start,\n",
    "                     encoding=\"latin-1\", dtype=str)\n",
    "    df = df.dropna(how=\"all\")\n",
    "\n",
    "    def htext(j):\n",
    "        a, b, c = hdr1[j].strip().lower(), hdr2[j].strip().lower(), hdr3[j].strip().lower()\n",
    "        return a, b, c, (a + \" | \" + b + \" | \" + c)\n",
    "\n",
    "    region_idx, belongs_idx = None, None\n",
    "    for j in range(maxlen):\n",
    "        a, b, c, _ = htext(j)\n",
    "        if region_idx is None and any(k in a+b+c for k in [\"gebiet\", \"bundesland\", \"land\"]):\n",
    "            region_idx = j\n",
    "        if belongs_idx is None and any(k in a+b+c for k in [\"gehört\", \"gehoert\", \"geh. zu\", \"gehört zu\"]):\n",
    "            belongs_idx = j\n",
    "    if region_idx is None: region_idx = 1\n",
    "    if belongs_idx is None: belongs_idx = 2\n",
    "\n",
    "    def is_zweit_count(j):\n",
    "        a, b, c, _ = htext(j)\n",
    "        txt = a + \" \" + b + \" \" + c\n",
    "        if \"zweit\" not in txt:\n",
    "            return False\n",
    "        if any(k in txt for k in [\"anteil\", \"quote\", \"%\", \"prozent\"]):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    party_cols = {}\n",
    "    tags = {\n",
    "        \"DIE LINKE\": [\"die linke\", \" linke \"],\n",
    "        \"SPD\": [\"spd\"], \"CDU\": [\"cdu\"], \"GRUENE\": [\"grüne\",\"gruene\",\"bündnis 90\",\"buendnis 90\"],\n",
    "        \"FDP\": [\"fdp\"], \"AfD\": [\"afd\"], \"CSU\": [\"csu\"],\n",
    "        \"BSW\": [\"bsw\"], \"FW\": [\"freie wähler\",\"freie waehler\",\"fw\"]\n",
    "    }\n",
    "    for j in range(maxlen):\n",
    "        if not is_zweit_count(j): continue\n",
    "        _, _, _, full = htext(j)\n",
    "        for party, needles in tags.items():\n",
    "            if any(n in full for n in needles):\n",
    "                party_cols.setdefault(party, []).append(j)\n",
    "\n",
    "    def pick_best(idxs):\n",
    "        if not idxs: return None\n",
    "        ranked = []\n",
    "        for j in idxs:\n",
    "            _, _, _, full = htext(j)\n",
    "            score = 0\n",
    "            if \"endg\" in full: score += 2\n",
    "            if \"vorl\" in full: score += 1\n",
    "            ranked.append((score, j))\n",
    "        ranked.sort(reverse=True)\n",
    "        return ranked[0][1]\n",
    "\n",
    "    party_first_idx = {p: pick_best(idxs) for p, idxs in party_cols.items() if idxs}\n",
    "    if party_first_idx.get(\"DIE LINKE\") is None:\n",
    "        suspects = [(j, htext(j)[3]) for j in range(maxlen) if \"zweit\" in htext(j)[3] and \"linke\" in htext(j)[3]]\n",
    "        raise KeyError(\"Could not locate DIE LINKE Zweitstimmen. Candidates:\\n\" +\n",
    "                       \"\\n\".join([f\"[{j}] {txt}\" for j, txt in suspects]))\n",
    "\n",
    "    region = df.iloc[:, region_idx].astype(str).str.strip()\n",
    "    belongs = df.iloc[:, belongs_idx].astype(str).str.strip()\n",
    "    die_linke = pd.to_numeric(df.iloc[:, party_first_idx[\"DIE LINKE\"]], errors=\"coerce\")\n",
    "\n",
    "    total_arrays = []\n",
    "    for party, j in party_first_idx.items():\n",
    "        if j is None: continue\n",
    "        s = pd.to_numeric(df.iloc[:, j], errors=\"coerce\")\n",
    "        total_arrays.append(s)\n",
    "    votes_total = sum(total_arrays) if total_arrays else die_linke.copy()\n",
    "\n",
    "    mask_state = (belongs == \"99\")\n",
    "    out = pd.DataFrame({\n",
    "        \"Region\": region[mask_state],\n",
    "        \"Votes_DieLinke\": die_linke[mask_state],\n",
    "        \"Votes_Total\": votes_total[mask_state],\n",
    "    }).dropna()\n",
    "\n",
    "    out = out.groupby(\"Region\", as_index=False).sum(numeric_only=True)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0b8e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Spending data (already created by your earlier function) ---\n",
    "spending_df = pd.read_csv(\n",
    "    \"outputs/spending_by_state/estimated_spending_by_state_die_linke.csv\"\n",
    ").rename(columns={\"State\": \"Region\"})\n",
    "\n",
    "# --- Votes from results_2025_German_election.csv ---\n",
    "votes_state = load_state_votes_from_results_robust(\n",
    "    \"/Users/karlahootz/Desktop/Jura/LLM/Bocconi-Kings College/Material Kings/Dissertation/case study/results_2025_German_election.csv\"\n",
    ")\n",
    "votes_state[\"Vote_Share\"] = votes_state[\"Votes_DieLinke\"] / votes_state[\"Votes_Total\"]\n",
    "\n",
    "# --- Merge ---\n",
    "merged_vote_share = spending_df.merge(\n",
    "    votes_state[[\"Region\", \"Vote_Share\"]], on=\"Region\", how=\"inner\"\n",
    ")\n",
    "\n",
    "# --- Plot + correlation ---\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = merged_vote_share[\"Estimated_Spend\"].astype(float).values\n",
    "y = merged_vote_share[\"Vote_Share\"].astype(float).values\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.scatter(x, y, s=80)\n",
    "m, b = np.polyfit(x, y, 1)\n",
    "plt.plot(np.linspace(min(x), max(x), 100), m*np.linspace(min(x), max(x), 100)+b, linestyle=\"--\")\n",
    "for xi, yi, lab in zip(x, y, merged_vote_share[\"Region\"]):\n",
    "    plt.text(xi, yi, lab, fontsize=8)\n",
    "plt.xlabel(\"Estimated Meta Ad Spend (EUR)\")\n",
    "plt.ylabel(\"Die Linke Vote Share (Zweitstimme)\")\n",
    "plt.title(\"Spending vs Vote Share (Bundesländer)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Pearson r:\", pearsonr(x,y))\n",
    "print(\"Spearman ρ:\", spearmanr(x,y))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}